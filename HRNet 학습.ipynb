{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 크기 차이가 나는건 일단 어찌해결할지 생각이 안남.  \n",
    "box를 자를 때 비율이 맞도록 자르는 방법?  \n",
    "근데 원래 이미지 자체가 비율이 안맞는데(누워있거나 서있거나) 어려울 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 augmentation은 쓰지 않고, 기본 성능을 측정해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test는 누워서 하는 동작이 절반인데 train에선 별로 없고, 사람도 다름  \n",
    "철봉 운동에서 일부 손이 짤린게 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 라이브러리 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from io import TextIOWrapper\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Sequence, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils\n",
    "import networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = Path(\"results/HRNet학습\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4  # transfer learning이니깐 좀 작게 주는게 좋을 것 같아서 1e-4\n",
    "BATCH_SIZE = 10\n",
    "START_EPOCH = 1\n",
    "SAM = False\n",
    "FOLDS = [1, 2, 3, 4, 5]\n",
    "HRNET_WIDTH = 48\n",
    "USE_L1 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = datetime.now()\n",
    "UID = f\"{n.year:04d}{n.month:02d}{n.day:02d}-{n.hour:02d}{n.minute:02d}{n.second:02d}\"\n",
    "SEED = 20210309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2021-03-10 21:01:30  INFO] 학습 시작\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(SEED, deterministic=False)\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "log = utils.CustomLogger(RESULT_DIR / f\"log_{UID}.log\", \"a\")\n",
    "log.info(\"학습 시작\")\n",
    "log.info(\"UID:\", UID)\n",
    "log.info(\"SEED:\", SEED)\n",
    "log.info(\"LR:\", LR)\n",
    "log.info(\"BATCH_SIZE:\", BATCH_SIZE)\n",
    "log.info(\"START_EPOCH:\", START_EPOCH)\n",
    "log.info(\"SAM:\", SAM)\n",
    "log.info(\"FOLDS:\", FOLDS)\n",
    "log.info(\"HRNET_WIDTH:\", HRNET_WIDTH)\n",
    "log.info(\"USE_L1:\", USE_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.array(sorted(list(Path(\"data/box2/train_imgs/\").glob(\"*.jpg\"))))\n",
    "test_imgs = np.array(sorted(list(Path(\"data/box2/test_imgs/\").glob(\"*.jpg\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = pd.read_csv(\"data/ori/train_df.csv\").to_numpy()[:, 1:].astype(np.float32)\n",
    "keypoints = np.stack([keypoints[:, 0::2], keypoints[:, 1::2]], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/box2/offset.json\", \"r\") as f:\n",
    "    offsets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, files, offsets, keypoints=None):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.offsets = offsets\n",
    "        self.keypoints = keypoints\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f = self.files[idx]\n",
    "        img = imageio.imread(f)\n",
    "        H, W, _ = img.shape\n",
    "        # TODO 가로로 긴 영상이면 가로 길이가 768이 되도록 만들기\n",
    "        ratio = torch.tensor([576 / W, 768 / H], dtype=torch.float32)\n",
    "        img = cv2.resize(img, (576, 768))\n",
    "        x = torch.as_tensor(img, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "        offset = torch.tensor(self.offsets[idx][\"boxes\"][:2], dtype=torch.int64)\n",
    "\n",
    "        if self.keypoints is not None:\n",
    "            keypoint = torch.tensor(self.keypoints[idx], dtype=torch.float32)\n",
    "            keypoint[:, 0] = (keypoint[:, 0] - offset[0]) * ratio[0] / 4\n",
    "            keypoint[:, 1] = (keypoint[:, 1] - offset[1]) * ratio[1] / 4\n",
    "            keypoint = keypoint.type(torch.int64)\n",
    "            # TODO: 나중에 augmentation 추가\n",
    "\n",
    "            \"\"\"# 좌표값 keypoint를 24차원 평면으로 변환\n",
    "            y = torch.zeros(24, 768 // 4, 576 // 4, dtype=torch.int64)\n",
    "            for i in range(24):\n",
    "                y[i, keypoint[i, 1] // 4, keypoint[i, 0] // 4] = 1\"\"\"\n",
    "            # 좌표값 keypoint를 1차원 벡터의 위치 값으로 변환\n",
    "            y = keypoint[:, 0] + keypoint[:, 1] * (576 // 4)\n",
    "\n",
    "            return f.name, x, offset, ratio, y\n",
    "        return f.name, x, offset, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_total = ImageDataset(train_imgs, offsets[\"train\"], keypoints)\n",
    "ds_test = ImageDataset(test_imgs, offsets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_kwargs = dict(batch_size=BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습은 crossentropy, 학습 중간에 RMSE 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = networks.PoseHighResolutionNet(width=HRNET_WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f\"networks/models/pose_hrnet_w{HRNET_WIDTH}_384x288.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = nn.Conv2d(32, 24, 1)\n",
    "with torch.no_grad():\n",
    "    final_layer.weight[:17] = model.final_layer.weight\n",
    "    final_layer.bias[:17] = model.final_layer.bias\n",
    "model.final_layer = final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAM:\n",
    "    optimizer = utils.SAM(model.parameters(), optim.AdamW, lr=LR)\n",
    "else:\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointLoss(nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        x = x.flatten(2).flatten(0, 1)\n",
    "        y = y.flatten(0, 1)\n",
    "        return F.cross_entropy(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointRMSE(nn.Module):\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, y):\n",
    "        W = x.size(3)\n",
    "        xp = x.flatten(2).argmax(2)\n",
    "        xx, xy = xp % W, xp // W\n",
    "        yx, yy = y % W, y // W\n",
    "        return 4 * ((xx - yx) ** 2 + (xy - yy) ** 2).type(torch.float32).mean().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_L1:\n",
    "    criterion = nn.L1Loss().cuda()\n",
    "else:\n",
    "    criterion = KeypointLoss().cuda()\n",
    "criterion_rmse = KeypointRMSE().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dl: DataLoader):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "\n",
    "    meanloss = utils.AverageMeter()\n",
    "    meanrmse = utils.AverageMeter()\n",
    "    results = {\"image\": [], \"loss\": [], \"rmse\": []}\n",
    "    with tqdm(total=len(dl.dataset), ncols=100, leave=False, file=sys.stdout, desc=f\"Train[{epoch:03d}]\") as t:\n",
    "        for f, x, offset, ratio, y in dl:\n",
    "            x_ = x.cuda()\n",
    "            y_ = y.cuda()\n",
    "            p_ = model(x_)\n",
    "            loss = criterion(p_, y_)\n",
    "            rmse = criterion_rmse(p_, y_)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if isinstance(optimizer, utils.SAM):\n",
    "                optimizer.first_step()\n",
    "                loss = criterion(model(x_), y_).backward()\n",
    "                optimizer.second_step()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            meanloss.update(loss.item())\n",
    "            meanrmse.update(rmse.item())\n",
    "            results[\"image\"].append(f)\n",
    "            results[\"loss\"].append(loss.item())\n",
    "            results[\"rmse\"].append(rmse.item())\n",
    "            t.set_postfix_str(f\"loss: {loss.item():.6f}, rmse: {rmse.item():.6f}\", refresh=False)\n",
    "            t.update(len(x))\n",
    "\n",
    "    return meanloss(), meanrmse(), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_loop(dl: DataLoader):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "\n",
    "    meanloss = utils.AverageMeter()\n",
    "    meanrmse = utils.AverageMeter()\n",
    "    results = {\"image\": [], \"loss\": [], \"rmse\": []}\n",
    "    with tqdm(total=len(dl.dataset), ncols=100, leave=False, file=sys.stdout, desc=f\"Valid[{epoch:03d}]\") as t:\n",
    "        for f, x, offset, ratio, y in dl:\n",
    "            x_ = x.cuda()\n",
    "            y_ = y.cuda()\n",
    "            p_ = model(x_)\n",
    "            loss = criterion(p_, y_)\n",
    "            rmse = criterion_rmse(p_, y_)\n",
    "\n",
    "            meanloss.update(loss.item())\n",
    "            meanrmse.update(rmse.item())\n",
    "            results[\"image\"].append(f)\n",
    "            results[\"loss\"].append(loss.item())\n",
    "            results[\"rmse\"].append(rmse.item())\n",
    "            t.set_postfix_str(f\"loss: {loss.item():.6f}, rmse: {rmse.item():.6f}\", refresh=False)\n",
    "            t.update(len(x))\n",
    "\n",
    "    return meanloss(), meanrmse(), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2021-03-10 21:04:39  INFO] Epoch: 001, loss: 8.104329 ; 6.365505, rmse 170.510487 ; 127.524917\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:07:43  INFO] Epoch: 002, loss: 5.429306 ; 4.890924, rmse 105.545063 ; 85.588395\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:10:47  INFO] Epoch: 003, loss: 4.388287 ; 4.283707, rmse 74.655548 ; 58.925705\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:13:51  INFO] Epoch: 004, loss: 3.894571 ; 4.053457, rmse 57.168282 ; 51.238611\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:16:55  INFO] Epoch: 005, loss: 3.588316 ; 3.917953, rmse 49.147511 ; 45.867757\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:19:58  INFO] Epoch: 006, loss: 3.348715 ; 3.830489, rmse 43.668880 ; 43.681314\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:23:02  INFO] Epoch: 007, loss: 3.140721 ; 3.799157, rmse 38.995623 ; 43.613693\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:26:07  INFO] Epoch: 008, loss: 2.947796 ; 3.776302, rmse 36.506101 ; 43.265954\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:29:10  INFO] Epoch: 009, loss: 2.774278 ; 3.805148, rmse 33.028875 ; 38.951607\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:32:14  INFO] Epoch: 010, loss: 2.600559 ; 3.842333, rmse 30.939193 ; 42.684259\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:35:17  INFO] Epoch: 011, loss: 2.445313 ; 3.893916, rmse 29.646412 ; 41.228156\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:38:21  INFO] Epoch: 012, loss: 2.307427 ; 3.950440, rmse 26.309018 ; 42.089248\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:41:24  INFO] Epoch: 013, loss: 2.165394 ; 4.053837, rmse 25.173396 ; 41.013390\u001b[0m\n",
      "Epoch    13: reducing learning rate of group 0 to 5.0000e-05.\n",
      "\u001b[34m[2021-03-10 21:44:28  INFO] Epoch: 014, loss: 1.996900 ; 4.109528, rmse 22.340074 ; 42.772401\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:47:31  INFO] Epoch: 015, loss: 1.907301 ; 4.180562, rmse 21.335672 ; 43.086238\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:50:35  INFO] Epoch: 016, loss: 1.833057 ; 4.284699, rmse 21.149687 ; 44.894270\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:53:38  INFO] Epoch: 017, loss: 1.770130 ; 4.355220, rmse 20.225972 ; 44.212310\u001b[0m\n",
      "\u001b[34m[2021-03-10 21:56:42  INFO] Epoch: 018, loss: 1.709349 ; 4.448518, rmse 18.789861 ; 46.367433\u001b[0m\n",
      "Epoch    18: reducing learning rate of group 0 to 2.5000e-05.\n",
      "\u001b[34m[2021-03-10 21:59:46  INFO] Epoch: 019, loss: 1.631785 ; 4.512962, rmse 18.677501 ; 45.111752\u001b[0m\n",
      "\u001b[34m[2021-03-10 22:02:49  INFO] Epoch: 020, loss: 1.595261 ; 4.561860, rmse 18.107705 ; 47.336434\u001b[0m\n",
      "\u001b[34m[2021-03-10 22:05:52  INFO] Epoch: 021, loss: 1.562970 ; 4.612757, rmse 17.762212 ; 46.775185\u001b[0m\n",
      "\u001b[34m[2021-03-10 22:08:55  INFO] Epoch: 022, loss: 1.519327 ; 4.685567, rmse 17.316053 ; 46.628786\u001b[0m\n",
      "\u001b[34m[2021-03-10 22:11:59  INFO] Epoch: 023, loss: 1.490382 ; 4.757844, rmse 16.980653 ; 47.909493\u001b[0m\n",
      "Epoch    23: reducing learning rate of group 0 to 1.2500e-05.\n",
      "\u001b[34m[2021-03-10 22:15:02  INFO] Epoch: 024, loss: 1.453112 ; 4.760090, rmse 17.714767 ; 48.161833\u001b[0m\n",
      "\u001b[34m[2021-03-10 22:18:06  INFO] Epoch: 025, loss: 1.438217 ; 4.809431, rmse 16.279458 ; 48.965808\u001b[0m\n",
      "\u001b[34m[2021-03-10 22:21:10  INFO] Epoch: 026, loss: 1.418668 ; 4.825378, rmse 16.935164 ; 49.911049\u001b[0m\n",
      "\u001b[34m[2021-03-10 22:24:13  INFO] Epoch: 027, loss: 1.409108 ; 4.856014, rmse 16.128108 ; 49.403282\u001b[0m\n",
      "                                                                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fbe9b6f1649a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESULT_DIR\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'loss-{UID}.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-3c746cc4cf44>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dl)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_idx, valid_idx = list(kf.split(ds_train_total))[FOLD - 1]\n",
    "\n",
    "ds_train = Subset(ds_train_total, train_idx)\n",
    "ds_valid = Subset(ds_train_total, valid_idx)\n",
    "dl_train = DataLoader(ds_train, **dl_kwargs, shuffle=True)\n",
    "dl_valid = DataLoader(ds_valid, **dl_kwargs, shuffle=False)\n",
    "\n",
    "best_loss = math.inf\n",
    "early_stop_cnt = 0\n",
    "\n",
    "for epoch in range(START_EPOCH, 999):\n",
    "    tloss, trmse, tres = train_loop(dl_train)\n",
    "    vloss, vrmse, vres = valid_loop(dl_valid)\n",
    "\n",
    "    # Logging\n",
    "    log.info(f'Epoch: {epoch:03d}, loss: {tloss:.6f} ; {vloss:.6f}, rmse {trmse:.6f} ; {vrmse:.6f}')\n",
    "    scheduler.step(vloss)\n",
    "\n",
    "    # Earlystop\n",
    "    if vloss < best_loss:\n",
    "        best_loss = vloss\n",
    "        early_stop_cnt = 0\n",
    "\n",
    "        with open(RESULT_DIR/f'loss-{UID}.json', 'w') as f:\n",
    "            json.dump({'train': tres, 'valid': vres}, f)\n",
    "\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "        }, RESULT_DIR/f'ckpt-{UID}.pth')\n",
    "    elif early_stop_cnt >= 10:\n",
    "        log.info(f'Stop training at epoch {epoch}.')\n",
    "        break\n",
    "    else:\n",
    "        early_stop_cnt +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "horizontal flip을 할 때 left right 구분이 있는 keypoint들만 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: criterion을 L1을 써보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
