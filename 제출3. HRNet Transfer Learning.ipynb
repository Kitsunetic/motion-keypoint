{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import multiprocessing\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "from typing import List, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from easydict import EasyDict\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed, deterministic=False):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = deterministic\n",
    "        torch.backends.cudnn.benchmark = not deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_list = [317, 869, 873, 877, 911, 1559, 1560, 1562, 1566, 1575]\n",
    "error_list += [1577, 1578, 1582, 1606, 1607, 1622, 1623, 1624, 1625]\n",
    "error_list += [1629, 3968, 4115, 4116, 4117, 4118, 4119, 4120, 4121]\n",
    "error_list += [4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130]\n",
    "error_list += [4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139]\n",
    "error_list += [4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148]\n",
    "error_list += [4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157]\n",
    "error_list += [4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166]\n",
    "error_list += [4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175]\n",
    "error_list += [4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184]\n",
    "error_list += [4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194]\n",
    "\n",
    "# 20210323 추가\n",
    "error_list += [1516, 1597, 2221, 2808, 2821, 3081, 3084, 3085, 3090, 3093, 3283, 3284]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HRNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "BN_MOMENTUM = 0.1\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HighResolutionModule(nn.Module):\n",
    "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels, num_channels, multi_scale_output=True):\n",
    "        super(HighResolutionModule, self).__init__()\n",
    "        self._check_branches(num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
    "\n",
    "        self.num_inchannels = num_inchannels\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        self.multi_scale_output = multi_scale_output\n",
    "\n",
    "        self.branches = self._make_branches(num_branches, blocks, num_blocks, num_channels)\n",
    "        self.fuse_layers = self._make_fuse_layers()\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def _check_branches(self, num_branches, blocks, num_blocks, num_inchannels, num_channels):\n",
    "        if num_branches != len(num_blocks):\n",
    "            error_msg = \"NUM_BRANCHES({}) <> NUM_BLOCKS({})\".format(num_branches, len(num_blocks))\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_channels):\n",
    "            error_msg = \"NUM_BRANCHES({}) <> NUM_CHANNELS({})\".format(num_branches, len(num_channels))\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_inchannels):\n",
    "            error_msg = \"NUM_BRANCHES({}) <> NUM_INCHANNELS({})\".format(num_branches, len(num_inchannels))\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.num_inchannels[branch_index],\n",
    "                    num_channels[branch_index] * block.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], stride, downsample))\n",
    "        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n",
    "        for i in range(1, num_blocks[branch_index]):\n",
    "            layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
    "        branches = []\n",
    "\n",
    "        for i in range(num_branches):\n",
    "            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))\n",
    "\n",
    "        return nn.ModuleList(branches)\n",
    "\n",
    "    def _make_fuse_layers(self):\n",
    "        if self.num_branches == 1:\n",
    "            return None\n",
    "\n",
    "        num_branches = self.num_branches\n",
    "        num_inchannels = self.num_inchannels\n",
    "        fuse_layers = []\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\n",
    "            fuse_layer = []\n",
    "            for j in range(num_branches):\n",
    "                if j > i:\n",
    "                    fuse_layer.append(\n",
    "                        nn.Sequential(\n",
    "                            nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False),\n",
    "                            nn.BatchNorm2d(num_inchannels[i]),\n",
    "                            nn.Upsample(scale_factor=2 ** (j - i), mode=\"nearest\"),\n",
    "                        )\n",
    "                    )\n",
    "                elif j == i:\n",
    "                    fuse_layer.append(None)\n",
    "                else:\n",
    "                    conv3x3s = []\n",
    "                    for k in range(i - j):\n",
    "                        if k == i - j - 1:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
    "                            conv3x3s.append(\n",
    "                                nn.Sequential(\n",
    "                                    nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n",
    "                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
    "                            conv3x3s.append(\n",
    "                                nn.Sequential(\n",
    "                                    nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n",
    "                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n",
    "                                    nn.ReLU(True),\n",
    "                                )\n",
    "                            )\n",
    "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
    "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
    "\n",
    "        return nn.ModuleList(fuse_layers)\n",
    "\n",
    "    def get_num_inchannels(self):\n",
    "        return self.num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_branches == 1:\n",
    "            return [self.branches[0](x[0])]\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            x[i] = self.branches[i](x[i])\n",
    "\n",
    "        x_fuse = []\n",
    "\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
    "            for j in range(1, self.num_branches):\n",
    "                if i == j:\n",
    "                    y = y + x[j]\n",
    "                else:\n",
    "                    y = y + self.fuse_layers[i][j](x[j])\n",
    "            x_fuse.append(self.relu(y))\n",
    "\n",
    "        return x_fuse\n",
    "\n",
    "\n",
    "class PoseHighResolutionNet(nn.Module):\n",
    "    def __init__(self, width=32, num_keypoints=17):\n",
    "        assert width in [32, 48], f\"PoseHighResolutionNet width must be in [32, 48] not {width}\"\n",
    "        self.width = width\n",
    "\n",
    "        block = BasicBlock\n",
    "        num_modules = [1, 4, 3]\n",
    "        num_branches = [2, 3, 4]\n",
    "        num_inchannels = [\n",
    "            [2 ** i * width * block.expansion for i in range(2)],\n",
    "            [2 ** i * width * block.expansion for i in range(3)],\n",
    "            [2 ** i * width * block.expansion for i in range(4)],\n",
    "        ]\n",
    "        self.pre_stage_channels = [256]\n",
    "\n",
    "        self.inplanes = 64\n",
    "        super(PoseHighResolutionNet, self).__init__()\n",
    "\n",
    "        # stem net\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(Bottleneck, 64, 4)\n",
    "\n",
    "        self.transition1 = self._make_transition_layer(num_inchannels[0])\n",
    "        self.stage2 = self._make_stage(block, num_modules[0], num_branches[0], num_inchannels[0])\n",
    "        self.transition2 = self._make_transition_layer(num_inchannels[1])\n",
    "        self.stage3 = self._make_stage(block, num_modules[1], num_branches[1], num_inchannels[1])\n",
    "        self.transition3 = self._make_transition_layer(num_inchannels[2])\n",
    "        self.stage4 = self._make_stage(block, num_modules[2], num_branches[2], num_inchannels[2], multi_scale_output=False)\n",
    "\n",
    "        self.final_layer = nn.Conv2d(self.pre_stage_channels[0], num_keypoints, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        self.finetune_step = 3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.num_branches[0]):\n",
    "            if self.transition1[i] is not None:\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list = self.stage2(x_list)\n",
    "        x = y_list[-1]\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.num_branches[1]):\n",
    "            if self.transition2[i] is not None:\n",
    "                x_list.append(self.transition2[i](x))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage3(x_list)\n",
    "        x = y_list[-1]\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.num_branches[2]):\n",
    "            if self.transition3[i] is not None:\n",
    "                x_list.append(self.transition3[i](x))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage4(x_list)\n",
    "        x = y_list[0]\n",
    "\n",
    "        x = self.final_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_transition_layer(self, num_channels_cur_layer):\n",
    "        num_channels_pre_layer = self.pre_stage_channels\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "\n",
    "        transition_layers = []\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    transition_layers.append(\n",
    "                        nn.Sequential(\n",
    "                            nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False),\n",
    "                            nn.BatchNorm2d(num_channels_cur_layer[i]),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                conv3x3s = []\n",
    "                for j in range(i + 1 - num_branches_pre):\n",
    "                    inchannels = num_channels_pre_layer[-1]\n",
    "                    outchannels = num_channels_cur_layer[i] if j == i - num_branches_pre else inchannels\n",
    "                    conv3x3s.append(\n",
    "                        nn.Sequential(\n",
    "                            nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),\n",
    "                            nn.BatchNorm2d(outchannels),\n",
    "                            nn.ReLU(inplace=True),\n",
    "                        )\n",
    "                    )\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
    "\n",
    "        return nn.ModuleList(transition_layers)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_stage(self, block, num_module, num_branch, num_inchannels, multi_scale_output=True):\n",
    "        modules = []\n",
    "        for i in range(num_module):\n",
    "            # multi_scale_output is only used last module\n",
    "            if not multi_scale_output and i == num_module - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "\n",
    "            modules.append(\n",
    "                HighResolutionModule(\n",
    "                    num_branch,\n",
    "                    block,\n",
    "                    [4 for _ in range(num_branch)],\n",
    "                    num_inchannels,\n",
    "                    [2 ** i * self.width for i in range(num_branch)],\n",
    "                    reset_multi_scale_output,\n",
    "                )\n",
    "            )\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        self.pre_stage_channels = num_inchannels\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.normal_(m.weight, std=0.001)\n",
    "                for name, _ in m.named_parameters():\n",
    "                    if name in [\"bias\"]:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.normal_(m.weight, std=0.001)\n",
    "                for name, _ in m.named_parameters():\n",
    "                    if name in [\"bias\"]:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def freeze_step1(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.final_layer.requires_grad_(True)\n",
    "        self.finetune_step = 1\n",
    "\n",
    "    def freeze_step2(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        self.final_layer.requires_grad_(False)\n",
    "        self.finetune_step = 2\n",
    "\n",
    "    def freeze_step3(self):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad_(True)\n",
    "        self.finetune_step = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorizontalFlipEx(A.HorizontalFlip):\n",
    "    swap_columns = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16), (18, 19), (22, 23)]\n",
    "\n",
    "    def apply_to_keypoints(self, keypoints, **params):\n",
    "        keypoints = super().apply_to_keypoints(keypoints, **params)\n",
    "\n",
    "        # left/right 키포인트들은 서로 swap해주기\n",
    "        for a, b in self.swap_columns:\n",
    "            temp1 = deepcopy(keypoints[a])\n",
    "            temp2 = deepcopy(keypoints[b])\n",
    "            keypoints[a] = temp2\n",
    "            keypoints[b] = temp1\n",
    "\n",
    "        return keypoints\n",
    "\n",
    "\n",
    "class VerticalFlipEx(A.VerticalFlip):\n",
    "    swap_columns = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16), (18, 19), (22, 23)]\n",
    "\n",
    "    def apply_to_keypoints(self, keypoints, **params):\n",
    "        keypoints = super().apply_to_keypoints(keypoints, **params)\n",
    "\n",
    "        # left/right 키포인트들은 서로 swap해주기\n",
    "        for a, b in self.swap_columns:\n",
    "            temp1 = deepcopy(keypoints[a])\n",
    "            temp2 = deepcopy(keypoints[b])\n",
    "            keypoints[a] = temp2\n",
    "            keypoints[b] = temp1\n",
    "\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reratio_box(roi: Tuple[float, float, float, float], ratio_limit=2.0):\n",
    "    # 극단적인 비율의 이미지는 조정해줄 필요가 있음\n",
    "    w, h = roi[2] - roi[0], roi[3] - roi[1]\n",
    "    dhl, dhr, dwl, dwr = 0, 0, 0, 0\n",
    "    if w / h > ratio_limit:\n",
    "        # w/(h+x) = l, x = w/l - h\n",
    "        dh = w / ratio_limit - h\n",
    "        dhl, dhr = math.floor(dh / 2), math.ceil(dh / 2)\n",
    "    elif h / w > ratio_limit:\n",
    "        # h/(w+x) = l, x = h/l - w\n",
    "        dw = h / ratio_limit - w\n",
    "        dwl, dwr = math.floor(dw / 2), math.ceil(dw / 2)\n",
    "    return dhl, dhr, dwl, dwr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint2box(keypoint, padding=0):\n",
    "    return np.array(\n",
    "        [\n",
    "            keypoint[:, 0].min() - padding,\n",
    "            keypoint[:, 1].min() - padding,\n",
    "            keypoint[:, 0].max() + padding,\n",
    "            keypoint[:, 1].max() + padding,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def keypoints2heatmaps(\n",
    "    k: torch.Tensor,\n",
    "    h=768 // 4,\n",
    "    w=576 // 4,\n",
    "    smooth=False,\n",
    "    smooth_size=3,\n",
    "    smooth_values=[0.1, 0.4, 0.8],\n",
    "):\n",
    "    k = k.type(torch.int64)\n",
    "    c = torch.zeros(k.size(0), h, w, dtype=torch.float32)\n",
    "    for i, (x, y) in enumerate(k):\n",
    "        if smooth:\n",
    "            for d, s in zip(range(smooth_size, 0, -1), smooth_values):\n",
    "                c[i, max(y - d, 0) : min(y + d, h), max(x - d, 0) : min(x + d, w)] = s\n",
    "        c[i, y, x] = 1.0\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, config, files, keypoints, augmentation):\n",
    "        super().__init__()\n",
    "        self.C = config\n",
    "        self.files = files\n",
    "        self.keypoints = keypoints\n",
    "\n",
    "        T = []\n",
    "        # T.append(A.Crop(0, 28, 1920, 1080 - 28))  # 1920x1080 --> 1920x1024\n",
    "        # T.append(A.Resize(512, 1024))\n",
    "        if augmentation:\n",
    "            # 중간에 기구로 잘리는 경우를 가장\n",
    "            T_ = []\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=0, p=1))\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=255, p=1))\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=128, p=1))\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=192, p=1))\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=64, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=0, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=255, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=128, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=192, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=64, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=0, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=255, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=128, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=192, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=64, p=1))\n",
    "            T.append(A.OneOf(T_))\n",
    "\n",
    "            # geomatric augmentations\n",
    "            # T.append(A.ShiftScaleRotate(border_mode=cv2.BORDER_CONSTANT))\n",
    "            T.append(A.ShiftScaleRotate())\n",
    "            T.append(HorizontalFlipEx())\n",
    "            T.append(VerticalFlipEx())\n",
    "            T.append(A.RandomRotate90())\n",
    "\n",
    "            T_ = []\n",
    "            T_.append(A.RandomBrightnessContrast(p=1))\n",
    "            T_.append(A.RandomGamma(p=1))\n",
    "            T_.append(A.RandomBrightness(p=1))\n",
    "            T_.append(A.RandomContrast(p=1))\n",
    "            T.append(A.OneOf(T_))\n",
    "\n",
    "            T_ = []\n",
    "            T_.append(A.MotionBlur(p=1))\n",
    "            T_.append(A.GaussNoise(p=1))\n",
    "            T.append(A.OneOf(T_))\n",
    "        if self.C.dataset.normalize:\n",
    "            if self.C.dataset.mean is not None and self.C.dataset.std is not None:\n",
    "                T.append(A.Normalize(self.C.dataset.mean, self.C.dataset.std))\n",
    "            else:\n",
    "                T.append(A.Normalize())\n",
    "        else:\n",
    "            T.append(A.Normalize((0, 0, 0), (1, 1, 1)))\n",
    "        T.append(ToTensorV2())\n",
    "\n",
    "        self.transform = A.Compose(\n",
    "            transforms=T,\n",
    "            bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"]),\n",
    "            keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # rotation/scale 등으로 keypoint가 화면 밖으로 나가면 exception 발생.\n",
    "        # 그럼 데이터 다시 만들어줌\n",
    "        while True:\n",
    "            try:\n",
    "                file = str(self.files[idx])\n",
    "                image = imageio.imread(file)\n",
    "\n",
    "                keypoint = self.keypoints[idx]\n",
    "                box = keypoint2box(keypoint, self.C.dataset.padding)\n",
    "                box = np.expand_dims(box, 0)\n",
    "                labels = np.array([0], dtype=np.int64)\n",
    "                a = self.transform(image=image, labels=labels, bboxes=box, keypoints=keypoint)\n",
    "\n",
    "                image = a[\"image\"]\n",
    "                # if not self.C.dataset.normalize:\n",
    "                #     image = image.type(torch.float) / 255.0\n",
    "                bbox = list(map(int, a[\"bboxes\"][0]))\n",
    "                keypoint = torch.tensor(a[\"keypoints\"], dtype=torch.float32)\n",
    "                image, keypoint, heatmap, ratio, offset = self._resize_image(image, bbox, keypoint)\n",
    "\n",
    "                return file, image, heatmap, ratio, offset\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    def _resize_image(self, image, bbox, keypoint):\n",
    "        \"\"\"\n",
    "        bbox크기만큼 이미지를 자르고, keypoint에 offset/ratio를 준다.\n",
    "        \"\"\"\n",
    "        dhl, dhr, dwl, dwr = reratio_box(bbox, ratio_limit=self.C.dataset.ratio_limit)\n",
    "        h, w = image.shape[1:3]\n",
    "        bbox[0] = max(bbox[0] - dwl, 0)\n",
    "        bbox[1] = max(bbox[1] - dhl, 0)\n",
    "        bbox[2] = min(bbox[2] + dwr, w)\n",
    "        bbox[3] = min(bbox[3] + dhr, h)\n",
    "\n",
    "        image = image[:, bbox[1] : bbox[3], bbox[0] : bbox[2]]\n",
    "        CD = self.C.dataset\n",
    "\n",
    "        # HRNet의 입력 이미지 크기로 resize\n",
    "        ratio = (CD.input_width / image.shape[2], CD.input_height / image.shape[1])\n",
    "        ratio = torch.tensor(ratio, dtype=torch.float32)\n",
    "        image = F.interpolate(image.unsqueeze(0), (CD.input_height, CD.input_width))[0]\n",
    "\n",
    "        # bbox만큼 빼줌\n",
    "        keypoint[:, 0] -= bbox[0]\n",
    "        keypoint[:, 1] -= bbox[1]\n",
    "\n",
    "        # 이미지를 resize해준 비율만큼 곱해줌\n",
    "        keypoint[:, 0] *= ratio[0]\n",
    "        keypoint[:, 1] *= ratio[1]\n",
    "        # TODO: 잘못된 keypoint가 있으면 고쳐줌\n",
    "\n",
    "        # HRNet은 1/4로 resize된 출력이 나오므로 4로 나눠줌\n",
    "        keypoint /= 4\n",
    "\n",
    "        # keypoint를 heatmap으로 변환\n",
    "        heatmap = keypoints2heatmaps(\n",
    "            keypoint,\n",
    "            CD.input_height // 4,\n",
    "            CD.input_width // 4,\n",
    "            smooth=self.C.dataset.smooth_heatmap.do,\n",
    "            smooth_size=self.C.dataset.smooth_heatmap.size,\n",
    "            smooth_values=self.C.dataset.smooth_heatmap.values,\n",
    "        )\n",
    "\n",
    "        offset = torch.tensor([bbox[0], bbox[1]], dtype=torch.float)\n",
    "\n",
    "        return image, keypoint, heatmap, ratio, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pose_datasets(C, fold):\n",
    "    total_imgs = np.array(sorted(list(C.dataset.train_dir.glob(\"*.jpg\"))))\n",
    "    df = pd.read_csv(C.dataset.target_file)\n",
    "    total_keypoints = df.to_numpy()[:, 1:].astype(np.float32)\n",
    "    total_keypoints = np.stack([total_keypoints[:, 0::2], total_keypoints[:, 1::2]], axis=2)\n",
    "\n",
    "    # 오류가 있는 데이터는 학습에서 제외\n",
    "    total_imgs_, total_keypoints_ = [], []\n",
    "    for i in range(len(total_imgs)):\n",
    "        if i not in error_list:\n",
    "            total_imgs_.append(total_imgs[i])\n",
    "            total_keypoints_.append(total_keypoints[i])\n",
    "    total_imgs = np.array(total_imgs_)\n",
    "    total_keypoints = np.array(total_keypoints_)\n",
    "\n",
    "    # KFold\n",
    "    if C.dataset.group_kfold:\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=C.seed)\n",
    "        # 파일 이름 앞 17자리를 group으로 이미지를 분류 (파일이 너무 잘 섞여도 안됨)\n",
    "        groups = []\n",
    "        last_group = 0\n",
    "        last_stem = total_imgs[0].name[:17]\n",
    "        for f in total_imgs:\n",
    "            stem = f.name[:17]\n",
    "            if stem == last_stem:\n",
    "                groups.append(last_group)\n",
    "            else:\n",
    "                last_group += 1\n",
    "                last_stem = stem\n",
    "                groups.append(last_group)\n",
    "        indices = list(skf.split(total_imgs, groups))\n",
    "    else:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=C.seed)\n",
    "        indices = list(kf.split(total_imgs))\n",
    "    train_idx, valid_idx = indices[fold - 1]\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    ds_train = KeypointDataset(\n",
    "        C,\n",
    "        total_imgs[train_idx],\n",
    "        total_keypoints[train_idx],\n",
    "        augmentation=True,\n",
    "    )\n",
    "    ds_valid = KeypointDataset(\n",
    "        C,\n",
    "        total_imgs[valid_idx],\n",
    "        total_keypoints[valid_idx],\n",
    "        augmentation=False,\n",
    "    )\n",
    "    dl_train = DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=C.dataset.batch_size,\n",
    "        num_workers=C.dataset.num_cpus,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    dl_valid = DataLoader(\n",
    "        ds_valid,\n",
    "        batch_size=C.dataset.batch_size,\n",
    "        num_workers=C.dataset.num_cpus,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return dl_train, dl_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    AverageMeter, referenced to https://dacon.io/competitions/official/235626/codeshare/1684\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if n > 0:\n",
    "            self.sum += val * n\n",
    "            self.cnt += n\n",
    "            self.avg = self.sum / self.cnt\n",
    "\n",
    "    def get(self):\n",
    "        return self.avg\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointLoss(nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        x = x.flatten(2).flatten(0, 1)\n",
    "        y = y.flatten(2).flatten(0, 1).argmax(1)\n",
    "        loss1 = F.cross_entropy(x, y)\n",
    "        return loss1\n",
    "\n",
    "\n",
    "class KeypointRMSE(nn.Module):\n",
    "    @torch.no_grad()\n",
    "    def forward(self, pred_heatmaps: torch.Tensor, real_heatmaps: torch.Tensor, ratios: torch.Tensor):\n",
    "        W = pred_heatmaps.size(3)\n",
    "        pred_positions = pred_heatmaps.flatten(2).argmax(2)\n",
    "        real_positions = real_heatmaps.flatten(2).argmax(2)\n",
    "        pred_positions = torch.stack((pred_positions // W, pred_positions % W), 2).type(torch.float32)\n",
    "        real_positions = torch.stack((real_positions // W, real_positions % W), 2).type(torch.float32)\n",
    "        # print(pred_positions.shape, real_positions.shape, ratios.shape)\n",
    "        pred_positions *= 4 / ratios.unsqueeze(1)  # position: (B, 24, 2), ratio: (B, 2)\n",
    "        real_positions *= 4 / ratios.unsqueeze(1)\n",
    "        loss = (pred_positions - real_positions).square().mean().sqrt()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainOutput:\n",
    "    def __init__(self):\n",
    "        self.loss = AverageMeter()\n",
    "        self.rmse = AverageMeter()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.loss = self.loss()\n",
    "        self.rmse = self.rmse()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseTrainer:\n",
    "    _tqdm_ = dict(ncols=100, leave=False, file=sys.stdout)\n",
    "\n",
    "    def __init__(self, config, fold, checkpoint=None):\n",
    "        self.C = config\n",
    "        self.fold = fold\n",
    "\n",
    "        # Create Network\n",
    "        if self.C.pose_model == \"HRNet-W32\":\n",
    "            width = 32\n",
    "        elif self.C.pose_model == \"HRNet-W48\":\n",
    "            width = 48\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        self.pose_model = PoseHighResolutionNet(width)\n",
    "        self.pose_model.load_state_dict(torch.load(f\"networks/models/pose_hrnet_w{width}_384x288.pth\"))\n",
    "\n",
    "        final_layer = nn.Conv2d(width, 24, 1)\n",
    "        with torch.no_grad():\n",
    "            final_layer.weight[:17] = self.pose_model.final_layer.weight\n",
    "            final_layer.bias[:17] = self.pose_model.final_layer.bias\n",
    "\n",
    "            if self.C.model_additional_weight:\n",
    "                # neck(17)은 left/right sholder(5, 6)과 nose(0)의 평균\n",
    "                # left/right palm(18, 19)는 left/right wrist(9, 10)을 복사\n",
    "                # spine2(20)은 left/right sholder(5, 6)과 left/right hip(11, 12)의 중앙\n",
    "                # spine1(21)은 left/right hip(11, 12)을 각각 1/3 + left/right sholder(5, 6)을 각각 1/6\n",
    "                # instep(22, 23)은 angle(15, 16)를 복사\n",
    "                final_layer.weight[17] = self.pose_model.final_layer.weight[[0, 5, 6]].clone().mean(0)\n",
    "                final_layer.bias[17] = self.pose_model.final_layer.bias[[0, 5, 6]].clone().mean(0)\n",
    "                final_layer.weight[18] = self.pose_model.final_layer.weight[9].clone()\n",
    "                final_layer.bias[18] = self.pose_model.final_layer.bias[9].clone()\n",
    "                final_layer.weight[19] = self.pose_model.final_layer.weight[10].clone()\n",
    "                final_layer.bias[19] = self.pose_model.final_layer.bias[10].clone()\n",
    "                final_layer.weight[20] = self.pose_model.final_layer.weight[[5, 6, 11, 12]].clone().mean(0)\n",
    "                final_layer.bias[20] = self.pose_model.final_layer.bias[[5, 6, 11, 12]].clone().mean(0)\n",
    "                final_layer.weight[21] = torch.cat(\n",
    "                    (\n",
    "                        self.pose_model.final_layer.weight[[11, 12]].clone() * 1 / 3,\n",
    "                        self.pose_model.final_layer.weight[[5, 6]].clone() * 6 / 1,\n",
    "                    )\n",
    "                ).mean(0)\n",
    "                final_layer.bias[21] = torch.cat(\n",
    "                    (\n",
    "                        self.pose_model.final_layer.bias[[11, 12]].clone() * 1 / 3,\n",
    "                        self.pose_model.final_layer.bias[[5, 6]].clone() * 6 / 1,\n",
    "                    )\n",
    "                ).mean(0)\n",
    "                final_layer.weight[22] = self.pose_model.final_layer.weight[15].clone()\n",
    "                final_layer.bias[22] = self.pose_model.final_layer.bias[15].clone()\n",
    "                final_layer.weight[23] = self.pose_model.final_layer.weight[16].clone()\n",
    "                final_layer.bias[23] = self.pose_model.final_layer.bias[16].clone()\n",
    "\n",
    "            self.pose_model.final_layer = final_layer\n",
    "        self.pose_model.cuda()\n",
    "\n",
    "        # Criterion\n",
    "        self.criterion = KeypointLoss().cuda()\n",
    "        self.criterion_rmse = KeypointRMSE().cuda()\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.AdamW(self.pose_model.parameters(), lr=self.C.train.lr)\n",
    "\n",
    "        self.epoch = 1\n",
    "        self.best_loss = math.inf\n",
    "        self.best_rmse = math.inf\n",
    "        self.earlystop_cnt = 0\n",
    "\n",
    "        # Dataset\n",
    "        self.dl_train, self.dl_valid = get_pose_datasets(self.C, self.fold)\n",
    "\n",
    "        # Load Checkpoint\n",
    "        if checkpoint is not None and Path(checkpoint).exists():\n",
    "            self.load(checkpoint)\n",
    "\n",
    "        # Scheduler\n",
    "        self.scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer, **self.C.train.scheduler.params)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": self.pose_model.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "                \"epoch\": self.epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"best_rmse\": self.best_rmse,\n",
    "                \"earlystop_cnt\": self.earlystop_cnt,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load(self, path):\n",
    "        print(\"Load pretrained\", path)\n",
    "        ckpt = torch.load(path)\n",
    "        self.pose_model.load_state_dict(ckpt[\"model\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        self.epoch = ckpt[\"epoch\"] + 1\n",
    "        self.best_loss = ckpt[\"best_loss\"]\n",
    "        self.best_rmse = ckpt[\"best_rmse\"]\n",
    "        self.earlystop_cnt = ckpt[\"earlystop_cnt\"]\n",
    "\n",
    "    def train_loop(self):\n",
    "        self.pose_model.train()\n",
    "\n",
    "        O = TrainOutput()\n",
    "        with tqdm(total=len(self.dl_train.dataset), desc=f\"Train {self.epoch:03d}\", **self._tqdm_) as t:\n",
    "            for files, imgs, target_heatmaps, ratios, offsets in self.dl_train:\n",
    "                imgs_, target_heatmaps_ = imgs.cuda(non_blocking=True), target_heatmaps.cuda(non_blocking=True)\n",
    "\n",
    "                # augmentation\n",
    "                if self.C.train.plus_augment.do:\n",
    "                    with torch.no_grad():\n",
    "                        c = self.C.train.plus_augment\n",
    "                        if c.downsample.do and random.random() <= c.downsample.p:\n",
    "                            h, w = imgs_.shape[2:]\n",
    "                            ratios[:, 0] = c.downsample.width / w * ratios[:, 0]\n",
    "                            ratios[:, 1] = c.downsample.height / h * ratios[:, 1]\n",
    "                            imgs_ = F.interpolate(imgs_, (c.downsample.height, c.downsample.width))\n",
    "                            target_heatmaps_ = F.interpolate(\n",
    "                                target_heatmaps_, (c.downsample.height // 4, c.downsample.width // 4)\n",
    "                            )\n",
    "\n",
    "                        if c.rotate.do and random.random() <= c.rotate.p:\n",
    "                            k = 3 if random.random() < 0.5 else 1\n",
    "                            ratios[:, 0], ratios[:, 1] = ratios[:, 1], ratios[:, 0]\n",
    "                            imgs_ = torch.rot90(imgs_, k, dims=(2, 3))\n",
    "                            target_heatmaps_ = torch.rot90(target_heatmaps_, k, dims=(2, 3))\n",
    "\n",
    "                pred_heatmaps_ = self.pose_model(imgs_)\n",
    "                loss = self.criterion(pred_heatmaps_, target_heatmaps_)\n",
    "                rmse = self.criterion_rmse(pred_heatmaps_, target_heatmaps_, ratios.cuda(non_blocking=True))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                O.loss.update(loss.item(), len(files))\n",
    "                O.rmse.update(rmse.item(), len(files))\n",
    "                t.set_postfix_str(f\"loss: {loss.item():.6f}, rmse: {rmse.item():.6f}\", refresh=False)\n",
    "                t.update(len(imgs))\n",
    "\n",
    "        return O.freeze()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_loop(self):\n",
    "        self.pose_model.eval()\n",
    "\n",
    "        O = TrainOutput()\n",
    "        with tqdm(total=len(self.dl_valid.dataset), desc=f\"Valid {self.epoch:03d}\", **self._tqdm_) as t:\n",
    "            for files, imgs, target_heatmaps, ratios, offsets in self.dl_valid:\n",
    "                imgs_, target_heatmaps_ = imgs.cuda(non_blocking=True), target_heatmaps.cuda(non_blocking=True)\n",
    "                pred_heatmaps_ = self.pose_model(imgs_)\n",
    "                loss = self.criterion(pred_heatmaps_, target_heatmaps_)\n",
    "                rmse = self.criterion_rmse(pred_heatmaps_, target_heatmaps_, ratios.cuda(non_blocking=True))\n",
    "\n",
    "                O.loss.update(loss.item(), len(files))\n",
    "                O.rmse.update(rmse.item(), len(files))\n",
    "                t.set_postfix_str(f\"loss: {loss.item():.6f}, rmse: {rmse.item():.6f}\", refresh=False)\n",
    "                t.update(len(imgs))\n",
    "\n",
    "        return O.freeze()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def callback(self, to: TrainOutput, vo: TrainOutput):\n",
    "        print(\n",
    "            f\"Epoch: {self.epoch:03d}/{self.C.train.max_epochs},\",\n",
    "            f\"loss: {to.loss:.6f};{vo.loss:.6f},\",\n",
    "            f\"rmse {to.rmse:.6f};{vo.rmse:.6f}\",\n",
    "        )\n",
    "\n",
    "        self.scheduler.step(vo.loss)\n",
    "\n",
    "        if self.best_loss > vo.loss or self.best_rmse > vo.rmse:\n",
    "            if self.best_loss > vo.loss:\n",
    "                self.best_loss = vo.loss\n",
    "            else:\n",
    "                self.best_rmse = vo.rmse\n",
    "\n",
    "            self.earlystop_cnt = 0\n",
    "            self.save(self.C.result_dir / f\"fold{self.fold}.pth\")\n",
    "        else:\n",
    "            self.earlystop_cnt += 1\n",
    "\n",
    "    def fit(self):\n",
    "        for self.epoch in range(self.epoch, self.C.train.max_epochs + 1):\n",
    "            if self.C.train.finetune.do:\n",
    "                if self.epoch <= self.C.train.finetune.step1_epochs:\n",
    "                    if self.pose_model.finetune_step != 1:\n",
    "                        print(\"Finetune step 1\")\n",
    "                        self.pose_model.freeze_step1()\n",
    "                elif self.epoch <= self.C.train.finetune.step2_epochs:\n",
    "                    if self.pose_model.finetune_step != 2:\n",
    "                        print(\"Finetune step 2\")\n",
    "                        self.pose_model.freeze_step2()\n",
    "                else:\n",
    "                    if self.pose_model.finetune_step != 3:\n",
    "                        print(\"Finetune step 3\")\n",
    "                        self.pose_model.freeze_step3()\n",
    "\n",
    "            to = self.train_loop()\n",
    "            vo = self.valid_loop()\n",
    "            self.callback(to, vo)\n",
    "\n",
    "            if self.earlystop_cnt > 10:\n",
    "                print(f\"Stop training at epoch\", self.epoch)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "__hrnet_train_config__ = \"\"\"\n",
    "pose_model: HRNet-W48\n",
    "model_additional_weight: true\n",
    "comment: null\n",
    "result_dir: results/submit/hrnet\n",
    "data_dir: data/ori\n",
    "debug: false\n",
    "seed: 20210309\n",
    "\n",
    "train:\n",
    "  max_epochs: 200\n",
    "  SAM: true\n",
    "  folds: \n",
    "    - 1\n",
    "    - 2\n",
    "    - 3\n",
    "    - 4\n",
    "    - 5\n",
    "  checkpoints: \n",
    "    - null\n",
    "    - null\n",
    "    - null\n",
    "    - null\n",
    "    - null\n",
    "  loss_type: ce # ce, bce, mse, mae, awing, sigmae, kldiv\n",
    "  \n",
    "  finetune:\n",
    "    do: true\n",
    "    step1_epochs: 3\n",
    "    step2_epochs: 6\n",
    "    \n",
    "  plus_augment:\n",
    "    do: true\n",
    "    downsample:\n",
    "      do: true\n",
    "      p: 0.2\n",
    "      width: 256\n",
    "      height: 256\n",
    "    rotate:\n",
    "      do: true\n",
    "      p: 0.4\n",
    "      left: true\n",
    "      right: true\n",
    "  \n",
    "  lr: 0.0001\n",
    "  scheduler:\n",
    "    type: ReduceLROnPlateau\n",
    "    params:\n",
    "      factor: 0.5\n",
    "      patience: 3\n",
    "      verbose: true\n",
    "  \n",
    "dataset:\n",
    "  train_dir: data/ori/train_imgs\n",
    "  target_file: data/ori/train_df.csv\n",
    "  test_dir: results/effdet-train/example/efficientdet-d7_multi5_flip_median_test_imgs\n",
    "  \n",
    "  scale_invariance: false\n",
    "  normalize: true\n",
    "  mean: [0.411, 0.420, 0.416]\n",
    "  std: [0.307, 0.303, 0.292]\n",
    "  smooth_heatmap: \n",
    "    do: true\n",
    "    size: 3\n",
    "    values: [0.1, 0.2, 0.5]\n",
    "  input_width: 512\n",
    "  input_height: 512\n",
    "  ratio_limit: 2.0\n",
    "  \n",
    "  batch_size: 15\n",
    "  num_cpus: 6\n",
    "  padding: 20\n",
    "  \n",
    "  group_kfold: false\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    C = EasyDict(yaml.load(__hrnet_train_config__, yaml.FullLoader))\n",
    "\n",
    "    for fold, checkpoint in zip(C.train.folds, C.train.checkpoints):\n",
    "        C = EasyDict(yaml.load(__hrnet_train_config__, yaml.FullLoader))\n",
    "        Path(C.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if C.dataset.num_cpus < 0:\n",
    "            C.dataset.num_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "        C.result_dir = Path(C.result_dir)\n",
    "        C.dataset.train_dir = Path(C.dataset.train_dir)\n",
    "        seed_everything(C.seed, deterministic=False)\n",
    "\n",
    "        trainer = PoseTrainer(C, fold, checkpoint)\n",
    "        trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune step 1\n",
      "Epoch: 001/5, loss: 9.158868;9.307481, rmse 81.590326;46.756194                                     \n",
      "Epoch: 002/5, loss: 8.995666;9.287218, rmse 84.094788;50.721443                                     \n",
      "Epoch: 003/5, loss: 9.009095;9.200552, rmse 85.877548;48.972834                                     \n",
      "Finetune step 2\n",
      "Epoch: 004/5, loss: 7.072980;5.931625, rmse 60.359309;40.384485                                     \n",
      "Epoch: 005/5, loss: 5.325669;4.976274, rmse 44.425532;34.272797                                     \n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5epoch만 돌려봤습니다.\n",
    "\n",
    "실제로 돌렸을 때 나온 로그는 아래와 같습니다.\n",
    "\n",
    "```\n",
    "[2021-04-03 01:50:42  INFO] Fold 1 , checkpoint None\n",
    "[2021-04-03 01:50:45  INFO] Finetune step 1\n",
    "[2021-04-03 01:52:06  INFO] Epoch: 001/200, loss: 9.111576;9.281687, rmse 81.787804;47.151906\n",
    "[2021-04-03 01:53:28  INFO] Epoch: 002/200, loss: 9.004133;9.202540, rmse 83.841997;48.828235\n",
    "[2021-04-03 01:54:49  INFO] Epoch: 003/200, loss: 9.009309;9.121378, rmse 84.239054;51.023133\n",
    "[2021-04-03 01:54:49  INFO] Finetune step 2\n",
    "[2021-04-03 01:57:26  INFO] Epoch: 004/200, loss: 6.473514;5.378668, rmse 57.224715;36.844983\n",
    "[2021-04-03 01:59:59  INFO] Epoch: 005/200, loss: 4.943404;4.682473, rmse 41.109905;28.716766\n",
    "[2021-04-03 02:02:34  INFO] Epoch: 006/200, loss: 4.474174;4.335109, rmse 32.775994;22.685376\n",
    "[2021-04-03 02:02:35  INFO] Finetune step 3\n",
    "[2021-04-03 02:05:12  INFO] Epoch: 007/200, loss: 4.064342;3.849286, rmse 27.056086;18.153155\n",
    "[2021-04-03 02:07:47  INFO] Epoch: 008/200, loss: 3.817954;3.699427, rmse 24.673332;15.462396\n",
    "[2021-04-03 02:10:19  INFO] Epoch: 009/200, loss: 3.656661;3.626052, rmse 22.458779;14.047837\n",
    "[2021-04-03 02:12:54  INFO] Epoch: 010/200, loss: 3.597842;3.542171, rmse 20.562469;12.546319\n",
    "[2021-04-03 02:15:30  INFO] Epoch: 011/200, loss: 3.534556;3.490196, rmse 19.450384;11.922026\n",
    "[2021-04-03 02:18:05  INFO] Epoch: 012/200, loss: 3.474406;3.451678, rmse 18.150288;11.867161\n",
    "[2021-04-03 02:20:39  INFO] Epoch: 013/200, loss: 3.417534;3.413972, rmse 17.463345;10.929338\n",
    "[2021-04-03 02:23:15  INFO] Epoch: 014/200, loss: 3.407891;3.377012, rmse 16.144659;10.885777\n",
    "[2021-04-03 02:25:51  INFO] Epoch: 015/200, loss: 3.368363;3.362036, rmse 15.968075;10.498726\n",
    "[2021-04-03 02:28:24  INFO] Epoch: 016/200, loss: 3.315080;3.338231, rmse 15.394482;10.415142\n",
    "[2021-04-03 02:30:58  INFO] Epoch: 017/200, loss: 3.294093;3.311528, rmse 14.878963;10.197726\n",
    "[2021-04-03 02:33:33  INFO] Epoch: 018/200, loss: 3.275022;3.283736, rmse 14.254614;9.979561\n",
    "[2021-04-03 02:36:05  INFO] Epoch: 019/200, loss: 3.243217;3.267931, rmse 14.406654;10.213960\n",
    "[2021-04-03 02:38:41  INFO] Epoch: 020/200, loss: 3.243431;3.249061, rmse 13.852117;9.934496\n",
    "[2021-04-03 02:41:16  INFO] Epoch: 021/200, loss: 3.221962;3.235899, rmse 13.751989;9.875229\n",
    "[2021-04-03 02:43:53  INFO] Epoch: 022/200, loss: 3.215765;3.223300, rmse 13.173394;9.770934\n",
    "[2021-04-03 02:46:30  INFO] Epoch: 023/200, loss: 3.196588;3.191802, rmse 13.132091;9.603020\n",
    "[2021-04-03 02:49:02  INFO] Epoch: 024/200, loss: 3.147130;3.198061, rmse 13.433540;9.733900\n",
    "[2021-04-03 02:51:38  INFO] Epoch: 025/200, loss: 3.143757;3.177124, rmse 13.000288;9.355708\n",
    "[2021-04-03 02:54:14  INFO] Epoch: 026/200, loss: 3.143037;3.175199, rmse 12.615048;9.502426\n",
    "[2021-04-03 02:56:47  INFO] Epoch: 027/200, loss: 3.126487;3.167524, rmse 12.853057;9.241065\n",
    "[2021-04-03 02:59:22  INFO] Epoch: 028/200, loss: 3.084297;3.151067, rmse 12.150601;9.099870\n",
    "[2021-04-03 03:01:56  INFO] Epoch: 029/200, loss: 3.097590;3.147912, rmse 12.353869;9.074561\n",
    "[2021-04-03 03:04:30  INFO] Epoch: 030/200, loss: 3.066993;3.131535, rmse 12.083944;9.089086\n",
    "[2021-04-03 03:07:05  INFO] Epoch: 031/200, loss: 3.068503;3.122327, rmse 12.273081;9.092817\n",
    "[2021-04-03 03:09:35  INFO] Epoch: 032/200, loss: 3.033859;3.116148, rmse 12.088444;8.912008\n",
    "[2021-04-03 03:12:10  INFO] Epoch: 033/200, loss: 3.053121;3.129031, rmse 11.630105;8.997702\n",
    "[2021-04-03 03:14:43  INFO] Epoch: 034/200, loss: 3.030079;3.107317, rmse 11.397275;9.422136\n",
    "[2021-04-03 03:17:22  INFO] Epoch: 035/200, loss: 3.066026;3.091151, rmse 11.552298;8.612168\n",
    "[2021-04-03 03:19:54  INFO] Epoch: 036/200, loss: 2.995404;3.072383, rmse 12.019486;9.076405\n",
    "[2021-04-03 03:22:29  INFO] Epoch: 037/200, loss: 3.013175;3.068880, rmse 11.397027;9.057779\n",
    "[2021-04-03 03:25:05  INFO] Epoch: 038/200, loss: 3.007169;3.072455, rmse 10.686960;8.785975\n",
    "[2021-04-03 03:27:40  INFO] Epoch: 039/200, loss: 2.997308;3.050728, rmse 11.668658;8.779771\n",
    "[2021-04-03 03:30:13  INFO] Epoch: 040/200, loss: 2.968577;3.074040, rmse 11.048011;8.814108\n",
    "[2021-04-03 03:32:47  INFO] Epoch: 041/200, loss: 2.980202;3.035634, rmse 11.401218;8.910084\n",
    "[2021-04-03 03:35:19  INFO] Epoch: 042/200, loss: 2.949863;3.032272, rmse 11.401427;8.884144\n",
    "[2021-04-03 03:37:52  INFO] Epoch: 043/200, loss: 2.957391;3.027880, rmse 11.089871;8.268735\n",
    "[2021-04-03 03:40:25  INFO] Epoch: 044/200, loss: 2.938866;3.029270, rmse 10.891933;8.695430\n",
    "[2021-04-03 03:43:00  INFO] Epoch: 045/200, loss: 2.932901;3.029656, rmse 10.767609;8.818323\n",
    "[2021-04-03 03:45:35  INFO] Epoch: 046/200, loss: 2.941626;2.985382, rmse 10.630344;8.607774\n",
    "[2021-04-03 03:48:13  INFO] Epoch: 047/200, loss: 2.928358;2.993131, rmse 10.385914;8.409465\n",
    "[2021-04-03 03:50:47  INFO] Epoch: 048/200, loss: 2.910414;2.989746, rmse 10.296080;8.713046\n",
    "[2021-04-03 03:53:22  INFO] Epoch: 049/200, loss: 2.918711;2.990455, rmse 10.196509;8.435738\n",
    "[2021-04-03 03:55:57  INFO] Epoch: 050/200, loss: 2.906180;2.997988, rmse 10.475929;8.856266\n",
    "[2021-04-03 03:58:30  INFO] Epoch: 051/200, loss: 2.884906;2.980430, rmse 10.497673;8.537921\n",
    "[2021-04-03 04:01:06  INFO] Epoch: 052/200, loss: 2.881751;2.960218, rmse 10.286178;8.477791\n",
    "[2021-04-03 04:03:38  INFO] Epoch: 053/200, loss: 2.849795;2.967345, rmse 10.253557;8.388203\n",
    "[2021-04-03 04:06:13  INFO] Epoch: 054/200, loss: 2.862772;3.007725, rmse 10.060478;8.661884\n",
    "[2021-04-03 04:08:47  INFO] Epoch: 055/200, loss: 2.853514;2.926684, rmse 10.045300;8.067089\n",
    "[2021-04-03 04:11:22  INFO] Epoch: 056/200, loss: 2.862386;2.951853, rmse 9.921272;8.132290\n",
    "[2021-04-03 04:13:58  INFO] Epoch: 057/200, loss: 2.845991;2.932593, rmse 9.744282;8.449003\n",
    "[2021-04-03 04:16:28  INFO] Epoch: 058/200, loss: 2.821307;2.940892, rmse 10.138666;8.318795\n",
    "[2021-04-03 04:19:00  INFO] Epoch: 059/200, loss: 2.819373;2.939199, rmse 9.835361;8.282974\n",
    "[2021-04-03 04:21:35  INFO] Epoch: 060/200, loss: 2.831455;2.909624, rmse 9.819294;8.265219\n",
    "[2021-04-03 04:24:08  INFO] Epoch: 061/200, loss: 2.808254;2.926813, rmse 9.614713;8.049121\n",
    "[2021-04-03 10:13:57  INFO] Epoch: 062/200, loss: 2.839149;2.925009, rmse 9.590933;8.190913\n",
    "[2021-04-03 10:16:36  INFO] Epoch: 063/200, loss: 2.820567;2.907754, rmse 9.898347;8.177191\n",
    "[2021-04-03 10:19:20  INFO] Epoch: 064/200, loss: 2.850035;2.893030, rmse 9.180195;7.893622\n",
    "[2021-04-03 10:22:02  INFO] Epoch: 065/200, loss: 2.813749;2.896620, rmse 9.565798;7.961446\n",
    "[2021-04-03 10:24:41  INFO] Epoch: 066/200, loss: 2.772795;2.921744, rmse 9.346566;8.290775\n",
    "[2021-04-03 10:27:20  INFO] Epoch: 067/200, loss: 2.779566;2.909194, rmse 9.299446;7.869686\n",
    "[2021-04-03 10:30:03  INFO] Epoch: 068/200, loss: 2.771226;2.887583, rmse 8.838923;7.770881\n",
    "[2021-04-03 10:32:45  INFO] Epoch: 069/200, loss: 2.753618;2.888313, rmse 9.089010;7.948496\n",
    "[2021-04-03 10:35:22  INFO] Epoch: 070/200, loss: 2.706607;2.888361, rmse 8.714147;7.950290\n",
    "[2021-04-03 10:38:01  INFO] Epoch: 071/200, loss: 2.721139;2.907770, rmse 8.587005;8.030171\n",
    "[2021-04-03 10:40:42  INFO] Epoch: 072/200, loss: 2.725513;2.888864, rmse 8.689077;8.068762\n",
    "[2021-04-03 10:43:21  INFO] Epoch: 073/200, loss: 2.713907;2.890966, rmse 8.590899;8.160121\n",
    "[2021-04-03 10:46:00  INFO] Epoch: 074/200, loss: 2.694574;2.888750, rmse 8.479927;8.128879\n",
    "[2021-04-03 10:48:42  INFO] Epoch: 075/200, loss: 2.721445;2.884164, rmse 8.446556;7.932857\n",
    "[2021-04-03 10:51:23  INFO] Epoch: 076/200, loss: 2.707748;2.882547, rmse 8.364045;7.859095\n",
    "[2021-04-03 10:54:02  INFO] Epoch: 077/200, loss: 2.677123;2.890484, rmse 8.232603;7.877231\n",
    "[2021-04-03 10:56:41  INFO] Epoch: 078/200, loss: 2.679331;2.880562, rmse 8.361204;7.804435\n",
    "[2021-04-03 10:59:21  INFO] Epoch: 079/200, loss: 2.679801;2.885669, rmse 8.131620;7.765136\n",
    "[2021-04-03 11:01:59  INFO] Epoch: 080/200, loss: 2.660931;2.881426, rmse 8.267631;7.762618\n",
    "[2021-04-03 11:04:41  INFO] Epoch: 081/200, loss: 2.677028;2.882562, rmse 8.227345;8.021825\n",
    "[2021-04-03 11:07:20  INFO] Epoch: 082/200, loss: 2.666284;2.877246, rmse 8.059798;7.742466\n",
    "[2021-04-03 11:10:04  INFO] Epoch: 083/200, loss: 2.670393;2.883696, rmse 8.084868;7.772282\n",
    "[2021-04-03 11:12:44  INFO] Epoch: 084/200, loss: 2.666446;2.871469, rmse 8.117327;7.752082\n",
    "[2021-04-03 11:15:22  INFO] Epoch: 085/200, loss: 2.638374;2.890071, rmse 8.350603;7.738865\n",
    "[2021-04-03 11:18:03  INFO] Epoch: 086/200, loss: 2.643916;2.887219, rmse 8.085594;7.742009\n",
    "[2021-04-03 11:20:43  INFO] Epoch: 087/200, loss: 2.647046;2.885289, rmse 8.043022;7.688209\n",
    "[2021-04-03 11:23:22  INFO] Epoch: 088/200, loss: 2.637412;2.884242, rmse 8.196427;7.693994\n",
    "[2021-04-03 11:26:01  INFO] Epoch: 089/200, loss: 2.611991;2.871550, rmse 7.818042;7.714978\n",
    "[2021-04-03 11:28:40  INFO] Epoch: 090/200, loss: 2.633182;2.891691, rmse 8.161644;7.736809\n",
    "[2021-04-03 11:31:18  INFO] Epoch: 091/200, loss: 2.606546;2.880741, rmse 7.874489;7.649662\n",
    "[2021-04-03 11:33:58  INFO] Epoch: 092/200, loss: 2.616876;2.867935, rmse 7.981494;7.721557\n",
    "[2021-04-03 11:36:35  INFO] Epoch: 093/200, loss: 2.583245;2.872302, rmse 8.077883;7.671826\n",
    "[2021-04-03 11:39:13  INFO] Epoch: 094/200, loss: 2.611010;2.906274, rmse 8.023121;7.635853\n",
    "[2021-04-03 11:41:52  INFO] Epoch: 095/200, loss: 2.598272;2.883841, rmse 7.902870;7.704837\n",
    "[2021-04-03 11:44:35  INFO] Epoch: 096/200, loss: 2.626818;2.871694, rmse 7.715516;7.743892\n",
    "[2021-04-03 11:47:11  INFO] Epoch: 097/200, loss: 2.572718;2.874291, rmse 8.090728;7.732247\n",
    "[2021-04-03 11:49:51  INFO] Epoch: 098/200, loss: 2.588042;2.876649, rmse 7.723771;7.708593\n",
    "[2021-04-03 11:52:30  INFO] Epoch: 099/200, loss: 2.597060;2.891991, rmse 7.452216;7.669222\n",
    "[2021-04-03 11:55:09  INFO] Epoch: 100/200, loss: 2.584952;2.888988, rmse 7.783365;7.791896\n",
    "[2021-04-03 11:57:47  INFO] Epoch: 101/200, loss: 2.564278;2.911695, rmse 7.660972;7.747816\n",
    "[2021-04-03 12:00:25  INFO] Epoch: 102/200, loss: 2.574284;2.876146, rmse 7.829700;7.727427\n",
    "[2021-04-03 12:03:01  INFO] Epoch: 103/200, loss: 2.553566;2.880476, rmse 7.702065;7.794234\n",
    "[2021-04-03 12:05:38  INFO] Epoch: 104/200, loss: 2.563121;2.875806, rmse 7.629126;7.695067\n",
    "[2021-04-03 12:08:17  INFO] Epoch: 105/200, loss: 2.550955;2.906066, rmse 7.737550;7.976671\n",
    "[2021-04-03 12:08:17  INFO] Stop training at epoch 105\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
