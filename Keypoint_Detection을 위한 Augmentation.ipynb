{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqxrdSx1yoxz"
      },
      "source": [
        "# Motion Keypoint Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DWSjEsmysjq"
      },
      "source": [
        "### Module Mount & Data Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Igb6uxYylZN"
      },
      "source": [
        "import cv2\r\n",
        "import glob\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "import random\r\n",
        "import shutil\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\r\n",
        "from math import cos, sin, pi\r\n",
        "from PIL import Image\r\n",
        "from tqdm import tqdm\r\n",
        "from tensorflow.keras import Sequential, Model\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\r\n",
        "from tensorflow.keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D, ZeroPadding2D, GlobalAveragePooling2D\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1QKh3Ogyx3n"
      },
      "source": [
        "# 경로 이동\r\n",
        "os.chdir('/content/drive/MyDrive/Colab/Dacon/Motion_Keypoint/data') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1iK4QksyzaP"
      },
      "source": [
        "# 해당 코드는 아래 Train, Valid Split 이후에 실행\r\n",
        "train = pd.read_csv('./train.csv')\r\n",
        "valid = pd.read_csv('./valid.csv')\r\n",
        "\r\n",
        "train_paths = glob.glob('./train/*.jpg')\r\n",
        "valid_paths = glob.glob('./valid/*.jpg')\r\n",
        "test_paths = glob.glob('./test_imgs/*.jpg')\r\n",
        "print(len(train_paths), len(valid_paths), len(test_paths))\r\n",
        "\r\n",
        "train_paths.sort()\r\n",
        "valid_paths.sort()\r\n",
        "test_paths.sort()\r\n",
        "\r\n",
        "train['path'] = train_paths\r\n",
        "valid['path'] = valid_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nujK_vpU2c7f"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7xqzmKCy1ed"
      },
      "source": [
        "### Train, Valid Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbnPv_MDwubs"
      },
      "source": [
        "이미 TrainSet, ValidSet을 나눠두셨으면 아래 코드는 넘어가시면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BbG_haXy5ia"
      },
      "source": [
        "# train, val folder 생성\r\n",
        "root_dir = '/content/drive/My Drive/Colab/Dacon/Motion_Keypoint/data'\r\n",
        "\r\n",
        "os.makedirs(root_dir +'/train')\r\n",
        "os.makedirs(root_dir +'/val')\r\n",
        "\r\n",
        "# validation용 파일은 10% 비율로 random sampling\r\n",
        "# random.seed() 넣으시면 복원 가능\r\n",
        "src = \"train_imgs\"\r\n",
        "all_filename = os.listdir(src)\r\n",
        "valid_filename = random.sample(all_filename, int(len(train_all) * 0.1))\r\n",
        "train_filename = [x for x in all_filename if x not in valid_filename]\r\n",
        "\r\n",
        "print(len(train_filename), len(valid_filename))\r\n",
        "\r\n",
        "train_filename = [src+'/'+ name for name in train_filename]\r\n",
        "valid_filename = [src+'/' + name for name in valid_filename]\r\n",
        "\r\n",
        "print('Total images: ', len(all_filename))\r\n",
        "print('Training: ', len(train_filename))\r\n",
        "print('Validation: ', len(valid_filename))\r\n",
        "\r\n",
        "# copy & paste images\r\n",
        "for name in tqdm(train_filename):\r\n",
        "    shutil.copy(name, 'train')\r\n",
        "\r\n",
        "for name in tqdm(valid_filename):\r\n",
        "    shutil.copy(name, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRisfwtNy7Cm"
      },
      "source": [
        "# train, valid folder 속 모든 이미지 파일 read & sort\r\n",
        "train_paths = glob.glob('./train/*.jpg')\r\n",
        "valid_paths = glob.glob('./valid/*.jpg')\r\n",
        "train_paths.sort()\r\n",
        "valid_paths.sort()\r\n",
        "\r\n",
        "train_filename = []\r\n",
        "valid_filename = []\r\n",
        "\r\n",
        "for t_paths in tqdm(train_paths):\r\n",
        "    filename = t_paths.split('/')[-1]\r\n",
        "    train_filename.append(filename)\r\n",
        "\r\n",
        "for v_paths in tqdm(valid_paths):\r\n",
        "    filename = v_paths.split('/')[-1]\r\n",
        "    valid_filename.append(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OY8OPlUy89l"
      },
      "source": [
        "# 각각의 train, valid 이미지들의 정보만을 담고 있는 DataFrame 생성\r\n",
        "train_df = train[train['image'].isin(train_filename)]\r\n",
        "train_df.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "valid_df = train[train['image'].isin(valid_filename)]\r\n",
        "valid_df.reset_index(inplace=True, drop=True)\r\n",
        "\r\n",
        "train_df.to_csv('train.csv', index=False)\r\n",
        "valid_df.to_csv('valid.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBw6V7y22cTB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91E1oNvW1Q8s"
      },
      "source": [
        "### 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyDdP3mLy-Qe"
      },
      "source": [
        "plt.figure(figsize=(40,20))\r\n",
        "count=1\r\n",
        "\r\n",
        "for i in np.random.randint(0,len(train_paths),5):\r\n",
        "    \r\n",
        "    plt.subplot(5,1, count)\r\n",
        "    \r\n",
        "    img_sample_path = train_paths[i]\r\n",
        "    img = Image.open(img_sample_path)\r\n",
        "    img_np = np.array(img)\r\n",
        "\r\n",
        "    keypoint = train.iloc[:,1:49] #위치 키포인트 하나씩 확인\r\n",
        "    keypoint_sample = keypoint.iloc[i, :]\r\n",
        "    \r\n",
        "    for j in range(0,len(keypoint.columns),2):\r\n",
        "        plt.plot(keypoint_sample[j], keypoint_sample[j+1],'rx')\r\n",
        "        plt.imshow(img_np)\r\n",
        "    \r\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXb-8lq02b3w"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVQe-V0xzBVX"
      },
      "source": [
        "### Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiFTgOgNw3yf"
      },
      "source": [
        "Augmentation 함수 정의에 앞서 parameter들을 미리 선언했습니다.\r\n",
        "\r\n",
        "pixel_shifts, rotation_angles과 같이 리스트 형태로 둔 부분은 \r\n",
        "\r\n",
        "아래 함수에서 forloop로 구현했기 때문에 추가적으로 값을 넣으면 더 많은 이미지들이 생성됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slvHoEeVy_kU"
      },
      "source": [
        "# Augmentation Setting\r\n",
        "pixel_shifts = [12]\r\n",
        "rotation_angles = [12]\r\n",
        "inc_brightness_ratio = 1.2\r\n",
        "dec_brightness_ratio = 0.8\r\n",
        "noise_ratio = 0.008"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuQc7OH-zDHV"
      },
      "source": [
        "# 좌우 반전\r\n",
        "def left_right_flip(images, keypoints):\r\n",
        "    flipped_keypoints = []\r\n",
        "    flipped_images = np.flip(images, axis=1)\r\n",
        "    for idx, sample_keypoints in enumerate(keypoints):\r\n",
        "        if idx%2 == 0:\r\n",
        "            flipped_keypoints.append(480.-sample_keypoints)\r\n",
        "        else:\r\n",
        "            flipped_keypoints.append(sample_keypoints)\r\n",
        "    return flipped_images, flipped_keypoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EIf0fxQzEvf"
      },
      "source": [
        "# 수직/수평 동시 이동\r\n",
        "# forloop에서 shift_x, shift_y 중 하나만 놓으면\r\n",
        "# 수직 또는 수평 이동만 따로 시행 가능\r\n",
        "def shift_images(images, keypoints):\r\n",
        "    # tensor -> numpy\r\n",
        "    images = images.numpy()\r\n",
        "    shifted_images = []\r\n",
        "    shifted_keypoints = []\r\n",
        "    for shift in pixel_shifts:   \r\n",
        "        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\r\n",
        "            # 이동할 matrix 생성\r\n",
        "            M = np.float32([[1,0,shift_x],[0,1,shift_y]])\r\n",
        "            shifted_keypoint = np.array([])\r\n",
        "            shifted_x_list = np.array([])\r\n",
        "            shifted_y_list = np.array([])\r\n",
        "            # 이미지 이동\r\n",
        "            shifted_image = cv2.warpAffine(images, M, (480,270), flags=cv2.INTER_CUBIC)\r\n",
        "            # 이동한만큼 keypoint 수정\r\n",
        "            for idx, point in enumerate(keypoints):\r\n",
        "                if idx%2 == 0: \r\n",
        "                    shifted_keypoint = np.append(shifted_keypoint, point+shift_x)\r\n",
        "                    shifted_x_list = np.append(shifted_x_list, point+shift_x)\r\n",
        "                else: \r\n",
        "                    shifted_keypoint =np.append(shifted_keypoint, point+shift_y)\r\n",
        "                    shifted_y_list = np.append(shifted_y_list, point+shift_y)\r\n",
        "            # 수정된 keypoint가 이미지 사이즈를 벗어나지 않으면 append\r\n",
        "            if np.all(0.0<shifted_x_list) and np.all(shifted_x_list<480) and np.all(0.0<shifted_y_list) and np.all(shifted_y_list<270):\r\n",
        "                shifted_images.append(shifted_image.reshape(270,480,3))\r\n",
        "                shifted_keypoints.append(shifted_keypoint)\r\n",
        "\r\n",
        "    return shifted_images, shifted_keypoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBPEXH-QzF1P"
      },
      "source": [
        "# 이미지 회전\r\n",
        "def rotate_augmentation(images, keypoints):\r\n",
        "    # tensor -> numpy\r\n",
        "    images = images.numpy()\r\n",
        "    rotated_images = []\r\n",
        "    rotated_keypoints = []\r\n",
        "    \r\n",
        "    for angle in rotation_angles:\r\n",
        "        for angle in [angle,-angle]:\r\n",
        "            # 회전할 matrix 생성\r\n",
        "            M = cv2.getRotationMatrix2D((240,135), angle, 1.0)\r\n",
        "            # cv2_imshow로는 문제없지만 추후 plt.imshow로 사진을 확인할 경우 black screen 생성...\r\n",
        "            # 혹시 몰라 matrix를 ndarray로 변환\r\n",
        "            M = np.array(M, dtype=np.float32)\r\n",
        "            angle_rad = -angle*pi/180\r\n",
        "            rotated_image = cv2.warpAffine(images, M, (480,270))\r\n",
        "            rotated_images.append(rotated_image)\r\n",
        "            \r\n",
        "            # keypoint를 copy하여 forloop상에서 값이 계속 없데이트 되는 것을 회피\r\n",
        "            rotated_keypoint = keypoints.copy()\r\n",
        "            rotated_keypoint[0::2] = rotated_keypoint[0::2] - 240\r\n",
        "            rotated_keypoint[1::2] = rotated_keypoint[1::2] - 135\r\n",
        "            \r\n",
        "            for idx in range(0,len(rotated_keypoint),2):\r\n",
        "                rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)\r\n",
        "                rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)\r\n",
        "\r\n",
        "            rotated_keypoint[0::2] = rotated_keypoint[0::2] + 240\r\n",
        "            rotated_keypoint[1::2] = rotated_keypoint[1::2] + 135\r\n",
        "            rotated_keypoints.append(rotated_keypoint)\r\n",
        "        \r\n",
        "    return rotated_images, rotated_keypoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7oC3mNHzH0d"
      },
      "source": [
        "# 이미지 해상도 조절\r\n",
        "def alter_brightness(images):\r\n",
        "    altered_brightness_images = []\r\n",
        "    inc_brightness_images = np.clip(images*inc_brightness_ratio, 0.0, 1.0)\r\n",
        "    dec_brightness_images = np.clip(images*dec_brightness_ratio, 0.0, 1.0)\r\n",
        "    altered_brightness_images.append(inc_brightness_images)\r\n",
        "    altered_brightness_images.append(dec_brightness_images)\r\n",
        "    return altered_brightness_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJTY9OnmzJZT"
      },
      "source": [
        "# Random 노이즈 추가\r\n",
        "def add_noise(images):\r\n",
        "    images = images.numpy()\r\n",
        "    noise = noise_ratio * np.random.randn(270,480,3)\r\n",
        "    noise = noise.astype(np.float32)\r\n",
        "    # 생성한 noise를 원본에 add\r\n",
        "    noisy_image = cv2.add(images, noise)\r\n",
        "    return noisy_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U-7F4Ct2bCI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcLJSpTBzQIq"
      },
      "source": [
        "### Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jN-xAAZzNAp"
      },
      "source": [
        "def trainGenerator():\r\n",
        "    # 원본 이미지 resize\r\n",
        "    for i in range(len(train)):\r\n",
        "        img = tf.io.read_file(train['path'][i]) # path(경로)를 통해 이미지 읽기\r\n",
        "        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\r\n",
        "        img = tf.image.resize(img, [270,480]) # 이미지 resize \r\n",
        "        img = img/255                         # 이미지 rescaling\r\n",
        "        target = train.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\r\n",
        "        target = target/4                     # image size를 1920x1080 -> 480x270으로 바꿔줬으므로 keypoint도 변경\r\n",
        "\r\n",
        "        yield (img, target)\r\n",
        "    \r\n",
        "    # horizontal flip\r\n",
        "    for i in range(len(train)):\r\n",
        "        img = tf.io.read_file(train['path'][i]) \r\n",
        "        img = tf.image.decode_jpeg(img, channels=3) \r\n",
        "        img = tf.image.resize(img, [270,480]) \r\n",
        "        img = img/255\r\n",
        "        target = train.iloc[:,1:49].iloc[i,:] \r\n",
        "        target = target/4\r\n",
        "        img, target = left_right_flip(img, target)\r\n",
        "        \r\n",
        "        yield (img, target)\r\n",
        "\r\n",
        "    # Horizontal & Vertical shift\r\n",
        "    for i in range(len(train)):\r\n",
        "        img = tf.io.read_file(train['path'][i])\r\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\r\n",
        "        img = tf.image.resize(img, [270,480])\r\n",
        "        img = img/255\r\n",
        "        target = train.iloc[:,1:49].iloc[i,:]\r\n",
        "        target = target/4\r\n",
        "        img_list, target_list = shift_images(img, target)\r\n",
        "        for shifted_img, shifted_target in zip(img_list, target_list):\r\n",
        "            \r\n",
        "            yield (shifted_img, shifted_target)\r\n",
        "\r\n",
        "    # Rotation\r\n",
        "    for i in range(len(train)):\r\n",
        "        img = tf.io.read_file(train['path'][i])\r\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\r\n",
        "        img = tf.image.resize(img, [270,480])\r\n",
        "        img = img/255\r\n",
        "        target = train.iloc[:,1:49].iloc[i,:]\r\n",
        "        target = target/4\r\n",
        "        img_list, target_list = rotate_augmentation(img, target)\r\n",
        "        for rotated_img, rotated_target in zip(img_list, target_list):\r\n",
        "            \r\n",
        "            yield (rotated_img, rotated_target)\r\n",
        "\r\n",
        "    # Alter_Brightness\r\n",
        "    for i in range(len(train)):\r\n",
        "        img = tf.io.read_file(train['path'][i])\r\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\r\n",
        "        img = tf.image.resize(img, [270,480])\r\n",
        "        img = img/255\r\n",
        "        target = train.iloc[:,1:49].iloc[i,:]\r\n",
        "        target = target/4\r\n",
        "        img_list = alter_brightness(img)\r\n",
        "        for altered_brightness_images in img_list:\r\n",
        "            \r\n",
        "            yield (altered_brightness_images, target)\r\n",
        "\r\n",
        "    # Adding_Noise\r\n",
        "    for i in range(len(train)):\r\n",
        "        img = tf.io.read_file(train['path'][i])\r\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\r\n",
        "        img = tf.image.resize(img, [270,480])\r\n",
        "        img = img/255\r\n",
        "        target = train.iloc[:,1:49].iloc[i,:]\r\n",
        "        target = target/4\r\n",
        "        noisy_img = add_noise(img)\r\n",
        "\r\n",
        "        yield (noisy_img, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNN_5hfszUKE"
      },
      "source": [
        "def validGenerator():\r\n",
        "    # 원본 이미지 resize\r\n",
        "    for i in range(len(valid)):\r\n",
        "        img = tf.io.read_file(valid['path'][i]) # path(경로)를 통해 이미지 읽기\r\n",
        "        img = tf.image.decode_jpeg(img, channels=3) # 경로를 통해 불러온 이미지를 tensor로 변환\r\n",
        "        img = tf.image.resize(img, [270,480]) # 이미지 resize \r\n",
        "        img = img/255                         # 이미지 rescaling\r\n",
        "        target = valid.iloc[:,1:49].iloc[i,:] # keypoint 뽑아주기\r\n",
        "        target = target/4                     # image size를 1920x1080 -> 480x270으로 바꿔줬으므로 keypoint도 변경\r\n",
        "\r\n",
        "        yield (img, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QLgYDKdzVsU"
      },
      "source": [
        "batch_size = 64\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_generator(trainGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\r\n",
        "train_dataset = train_dataset.batch(batch_size).prefetch(1)\r\n",
        "valid_dataset = tf.data.Dataset.from_generator(validGenerator, (tf.float32, tf.float32), (tf.TensorShape([270,480,3]),tf.TensorShape([48])))\r\n",
        "valid_dataset = valid_dataset.batch(batch_size).prefetch(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI2Z6cVf2Z6L"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be05vP7UzYVO"
      },
      "source": [
        "### Baseline Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72xitbBV1ecB"
      },
      "source": [
        "해당 Baseline Model은 'Data Augmentation for Facial Keypoint Detection'에 나온 구조를 그대로 사용했고 callbacks만 추가했습니다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBuuYZakzc64"
      },
      "source": [
        "# Callback 설정\r\n",
        "earlystop = EarlyStopping(patience=7)\r\n",
        "learning_rate_reduction=ReduceLROnPlateau(\r\n",
        "                        monitor= \"val_loss\", \r\n",
        "                        patience = 2, \r\n",
        "                        factor = 0.85, \r\n",
        "                        min_lr=1e-7,\r\n",
        "                        verbose=1)\r\n",
        "\r\n",
        "model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \r\n",
        "        filepath=\"./baseline_with_augmentation.h5\", #모델 파일 경로\r\n",
        "        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\r\n",
        "        save_best_only=True)\r\n",
        "\r\n",
        "callbacks = [earlystop, learning_rate_reduction, model_check]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwJFkbU-zesm"
      },
      "source": [
        "# Model Structure\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(270,480,3)))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "\r\n",
        "model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "\r\n",
        "model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "\r\n",
        "model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "\r\n",
        "model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "\r\n",
        "model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\r\n",
        "model.add(LeakyReLU(alpha = 0.1))\r\n",
        "model.add(BatchNormalization())\r\n",
        "\r\n",
        "\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512,activation='relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(48))\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkgEfM5dzgLM"
      },
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001), \r\n",
        "              loss='mean_squared_error',\r\n",
        "              metrics=['mae'])\r\n",
        "\r\n",
        "history = model.fit(train_dataset,\r\n",
        "                    epochs=1000,\r\n",
        "                    validation_data=valid_dataset,\r\n",
        "                    callbacks = callbacks,\r\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWg9YNvAzkuP"
      },
      "source": [
        "### Pre-trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjhKWL8X2hRZ"
      },
      "source": [
        "아래는 tensorflow.keras.applications의 pre-trained model 사용 예시입니다.\r\n",
        "\r\n",
        "pre-trained model에 분류기만 적절하게 달아주시고\r\n",
        "\r\n",
        "미세 조정시 layers는 인덱싱을 통해 일부만 동결을 해제하실 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m5YkthhzjoS"
      },
      "source": [
        "# tensorflow.keras.applications 사용 예시\r\n",
        "from tensorflow.keras import models\r\n",
        "from tensorflow.keras.applications import ResNet152V2 \r\n",
        "\r\n",
        "resnet152 = ResNet152V2(weights ='imagenet', include_top = False, \r\n",
        "                       input_shape = (270,480,3))\r\n",
        "\r\n",
        "earlystop = EarlyStopping(patience=7)\r\n",
        "learning_rate_reduction=ReduceLROnPlateau(\r\n",
        "                        monitor= \"val_loss\", \r\n",
        "                        patience = 3, \r\n",
        "                        factor = 0.85, \r\n",
        "                        min_lr=1e-7,\r\n",
        "                        verbose=1)\r\n",
        "\r\n",
        "model_check = ModelCheckpoint( #에포크마다 현재 가중치를 저장    \r\n",
        "        filepath=\"./resnet152.h5\", #모델 파일 경로\r\n",
        "        monitor='val_loss',  # val_loss 가 좋아지지 않으면 모델 파일을 덮어쓰지 않음.\r\n",
        "        save_best_only=True)\r\n",
        "\r\n",
        "callbacks = [earlystop, learning_rate_reduction, model_check]\r\n",
        "\r\n",
        "# layer 동결 해제(일부 or 전체)\r\n",
        "for layer in resnet152.layers:\r\n",
        "    layer.trainable = True\r\n",
        "\r\n",
        "model = models.Sequential()\r\n",
        "model.add(resnet152)\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(512, activation='relu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(48)) \r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(loss='mean_squared_error',\r\n",
        "                optimizer=Adam(learning_rate=0.0001),\r\n",
        "                metrics=['mae'])\r\n",
        "\r\n",
        "history = model.fit(train_dataset,\r\n",
        "                    epochs=150,\r\n",
        "                    validation_data=valid_dataset,\r\n",
        "                    callbacks = callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJUafQsF3MXX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqWlwWUuz09O"
      },
      "source": [
        "### Load TestSet & Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv_YAgEKz0mB"
      },
      "source": [
        "test_paths = glob.glob('./test_imgs/*.jpg')\r\n",
        "test_paths.sort()\r\n",
        "X_test=[]\r\n",
        "\r\n",
        "for test_path in tqdm(test_paths):\r\n",
        "    img=tf.io.read_file(test_path)\r\n",
        "    img=tf.image.decode_jpeg(img, channels=3)\r\n",
        "    img=tf.image.resize(img, [270,480])\r\n",
        "    img=img/255\r\n",
        "    X_test.append(img)\r\n",
        "\r\n",
        "X_test=tf.stack(X_test, axis=0)\r\n",
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJOpC-mX0BNR"
      },
      "source": [
        "pred = saved_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDNjBKqI3OTv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_u95-gO0DaT"
      },
      "source": [
        "### Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETUGRT4Y3N0w"
      },
      "source": [
        "Baseline Model대로 학습하시면 Public 리더보드 기준으로 75점 전후로 나옵니다.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0bx89ky0ErL"
      },
      "source": [
        "submission = pd.read_csv('./sample_submission.csv')\r\n",
        "submission.iloc[:,1:] = pred * 4     # image size를 1920x1080 -> 480x270으로 바꿔서 예측했으므로 * 4\r\n",
        "# submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ps6i8sV0WUV"
      },
      "source": [
        "submission.to_csv('baseline.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbV1aSFK3cL1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_4HAZqu3kP0"
      },
      "source": [
        "### 예측 결과 시각화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PosqFT6L3cyV"
      },
      "source": [
        "코드를 실행하시면 Keypoint들이 사람 몸 안으로 들어오는 것을 확인하실 수는 있지만\r\n",
        "\r\n",
        "원하는대로 각 위치에 위치해 있지 않은 것을 확인할 수 있습니다.\r\n",
        "\r\n",
        "좀 더 Model의 성능을 향상해보시거나 혹은 SOTA 모형을 사용하시길 추천드립니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbBr-hji0bdo"
      },
      "source": [
        "# 예측 결과 시각화\r\n",
        "n = random.randint(0, 1600)\r\n",
        "predicted_keypoint = submission.iloc[n,1:49]\r\n",
        "predicted_keypoint = np.array(predicted_keypoint)\r\n",
        "img = Image.open(test_paths[n])\r\n",
        "plt.imshow(img)\r\n",
        "plt.scatter(predicted_keypoint[0::2], predicted_keypoint[1::2], marker='x')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}