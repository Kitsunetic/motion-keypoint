{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from io import TextIOWrapper\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Sequence, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = torch.tensor((0.485, 0.456, 0.406), dtype=torch.float32).reshape(3, 1, 1)\n",
    "STD = torch.tensor((0.229, 0.224, 0.225), dtype=torch.float32).reshape(3, 1, 1)\n",
    "\n",
    "PAD = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, files, keypoints=None, padding=40):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.keypoints = keypoints\n",
    "        self.padding = padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f = self.files[idx]\n",
    "        img = imageio.imread(f)\n",
    "        x = img[38:1062, 374:1526]  # 1152x1024\n",
    "        x = torch.tensor(x, dtype=torch.float32).permute(2, 0, 1).div(255.0)\n",
    "        x = (x - MEAN) / STD\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = sorted(list(Path(\"data/ori/train_imgs/\").glob(\"*.jpg\")))\n",
    "test_imgs = sorted(list(Path(\"data/ori/test_imgs/\").glob(\"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ImageDataset(train_imgs)\n",
    "ds_test = ImageDataset(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=20, num_workers=8, shuffle=False)\n",
    "dl_test = DataLoader(ds_test, batch_size=20, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = networks.EfficientDet(\"efficientdet-d7\", pretrained=True)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_train = Path(\"data/box_effdet/train_imgs\")\n",
    "dir_test = Path(\"data/box_effdet/test_imgs\")\n",
    "dir_train_keypoint = Path(\"data/box_effdet/train_keypoints\")\n",
    "dir_train.mkdir(parents=True, exist_ok=True)\n",
    "dir_test.mkdir(parents=True, exist_ok=True)\n",
    "dir_train_keypoint.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/box_effdet/train_df.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(\"data/ori/train_df.csv\", \"data/box_effdet/train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/box_effdet/train_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 4195/4195 [08:10<00:00,  8.56it/s, 642-2-4-31-Z148_E-0000031.jpg]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "i = 0\n",
    "result_train = []\n",
    "with tqdm(total=len(dl_train.dataset), ncols=100, file=sys.stdout) as t:\n",
    "    for xs in dl_train:\n",
    "        xs_ = xs.cuda()\n",
    "        outs = model(xs_)\n",
    "        for x, out in zip(xs, outs):\n",
    "            t.set_postfix_str(train_imgs[i].name)\n",
    "\n",
    "            # 사람 주변에 약간의 영역을 만들어줌\n",
    "            box = utils.get_single_person_rois(out)\n",
    "            box[0] -= PAD\n",
    "            box[1] -= PAD\n",
    "            box[2] += PAD\n",
    "            box[3] += PAD\n",
    "            out = ((x * STD + MEAN) * 255.0).permute(1, 2, 0).type(torch.uint8).numpy()\n",
    "\n",
    "            # 키포인트 변환을 위한 메타데이터\n",
    "            offset = [box[0] + 374, box[1] + 38]\n",
    "\n",
    "            # 키포인트가 정상적인 위치에 들어있는지 검사\n",
    "            keypoint = df.iloc[i, 1:].values.astype(np.float32)\n",
    "            keypoint = np.stack([keypoint[0::2], keypoint[1::2]], 1)\n",
    "            keypoint[:, 0] -= offset[0]\n",
    "            keypoint[:, 1] -= offset[1]\n",
    "            W = box[2] - box[0]\n",
    "            H = box[3] - box[1]\n",
    "\n",
    "            if (keypoint[:, 0] < 0).any():\n",
    "                box[0] += keypoint[:, 0].min() - 10\n",
    "                offset[0] += keypoint[:, 0].min().item() - 10\n",
    "                keypoint[:, 0] += -keypoint[:, 0].min() + 10\n",
    "            if (keypoint[:, 1] < 0).any():\n",
    "                box[1] += keypoint[:, 1].min() - 10\n",
    "                offset[1] += keypoint[:, 1].min() - 10\n",
    "                keypoint[:, 1] += -keypoint[:, 1].min().item() + 10\n",
    "            if (keypoint[:, 0] >= box[2] - box[0]).any():\n",
    "                box[2] += keypoint[:, 0].max() - W + 10\n",
    "            if (keypoint[:, 1] >= box[3] - box[1]).any():\n",
    "                box[3] += keypoint[:, 1].max() - H + 10\n",
    "\n",
    "            offset = np.array(offset).tolist()\n",
    "            result_train.append({\"image\": train_imgs[i].name, \"boxes\": offset})\n",
    "\n",
    "            out = out[box[1] : box[3], box[0] : box[2]]\n",
    "            imageio.imwrite(dir_train / train_imgs[i].name, out)\n",
    "\n",
    "            # 키포인트를 입힌 이미지도 모두 출력.\n",
    "            # 키포인트가 제대로 잡혀있는지 확인\n",
    "            keypoint_img = utils.draw_keypoints(out, keypoint)\n",
    "            imageio.imwrite(dir_train_keypoint / train_imgs[i].name, keypoint_img)\n",
    "\n",
    "            i += 1\n",
    "            t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 1600/1600 [02:57<00:00,  8.99it/s, 786-3-5-41-Z94_E-0000031.jpg]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "i = 0\n",
    "result_test = []\n",
    "with tqdm(total=len(dl_test.dataset), ncols=100, file=sys.stdout) as t:\n",
    "    for xs in dl_test:\n",
    "        xs_ = xs.cuda()\n",
    "        outs_ = model(xs_)\n",
    "        for x, p in zip(xs, outs_):\n",
    "            t.set_postfix_str(test_imgs[i].name)\n",
    "\n",
    "            # 사람 주변에 약간의 영역을 만들어줌\n",
    "            box = utils.get_single_person_rois(out)\n",
    "            box[0] -= PAD\n",
    "            box[1] -= PAD\n",
    "            box[2] += PAD\n",
    "            box[3] += PAD\n",
    "            out = ((x * STD + MEAN) * 255.0).permute(1, 2, 0).type(torch.uint8).numpy()\n",
    "            out = out[box[1] : box[3], box[0] : box[2]]\n",
    "            imageio.imwrite(dir_test / test_imgs[i].name, out)\n",
    "\n",
    "            # 키포인트 변환을 위한 메타데이터\n",
    "            offset = [box[0] + 374, box[1] + 38]\n",
    "            offset = np.array(offset).tolist()\n",
    "            result_test.append({\"image\": test_imgs[i].name, \"boxes\": offset})\n",
    "\n",
    "            i += 1\n",
    "            t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in result_train:\n",
    "    item[\"boxes\"] = np.array(item[\"boxes\"]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/box_effdet/offset.json\", \"w\") as f:\n",
    "    json.dump({\"train\": result_train, \"test\": result_test}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
