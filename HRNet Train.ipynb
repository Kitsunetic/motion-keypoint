{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Sequence, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networks\n",
    "import utils\n",
    "from error_list import error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_MODEL = \"HRNet-W48\"\n",
    "RESULT_DIR = Path(\"results/hrnet\")\n",
    "DATA_DIR = Path(\"data/ori\")\n",
    "\n",
    "LR = 1e-4  # transfer learning이니깐 좀 작게 주는게 좋을 것 같아서 1e-4\n",
    "BATCH_SIZE = 10\n",
    "START_EPOCH = 1\n",
    "SAM = False\n",
    "FOLD = 1\n",
    "PADDING = 30\n",
    "ADD_JOINT_LOSS = False\n",
    "\n",
    "DEBUG = False\n",
    "STEP1_EPOCHS = 10\n",
    "STEP2_EPOCHS = 20\n",
    "STEP3_EPOCHS = 500\n",
    "if DEBUG:\n",
    "    STEP1_EPOCHS = 2\n",
    "    STEP2_EPOCHS = 5\n",
    "    STEP3_EPOCHS = 10\n",
    "\n",
    "INPUT_WIDTH = 576  # 288\n",
    "INPUT_HEIGHT = 768  # 384\n",
    "\n",
    "CHECKPOINT_PATH = None\n",
    "\n",
    "n = datetime.now()\n",
    "UID = f\"{n.year:04d}{n.month:02d}{n.day:02d}-{n.hour:02d}{n.minute:02d}{n.second:02d}\"\n",
    "SEED = 20210309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2021-03-21 21:38:17  INFO] 학습 시작\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] POSE_MODEL: HRNet-W48\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] UID: 20210321-213817\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] SEED: 20210309\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] LR: 0.0001\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] BATCH_SIZE: 10\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] START_EPOCH: 1\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] SAM: False\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] FOLD: 1\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] PADDING: 30\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] ADD_JOINT_LOSS: False\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] DEBUG: False\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] STEP1_EPOCHS: 10\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] STEP2_EPOCHS: 20\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] STEP3_EPOCHS: 500\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] INPUT_WIDTH: 576\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] INPUT_HEIGHT: 768\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:38:17  INFO] CHECKPOINT_PATH: None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "utils.seed_everything(SEED, deterministic=False)\n",
    "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "log = utils.CustomLogger(RESULT_DIR / f\"log_{UID}.log\", \"a\")\n",
    "log.info(\"학습 시작\")\n",
    "log.info(\"POSE_MODEL:\", POSE_MODEL)\n",
    "log.info(\"UID:\", UID)\n",
    "log.info(\"SEED:\", SEED)\n",
    "log.info(\"LR:\", LR)\n",
    "log.info(\"BATCH_SIZE:\", BATCH_SIZE)\n",
    "log.info(\"START_EPOCH:\", START_EPOCH)\n",
    "log.info(\"SAM:\", SAM)\n",
    "log.info(\"FOLD:\", FOLD)\n",
    "log.info(\"PADDING:\", PADDING)\n",
    "log.info(\"ADD_JOINT_LOSS:\", ADD_JOINT_LOSS)\n",
    "log.info(\"DEBUG:\", DEBUG)\n",
    "log.info(\"STEP1_EPOCHS:\", STEP1_EPOCHS)\n",
    "log.info(\"STEP2_EPOCHS:\", STEP2_EPOCHS)\n",
    "log.info(\"STEP3_EPOCHS:\", STEP3_EPOCHS)\n",
    "log.info(\"INPUT_WIDTH:\", INPUT_WIDTH)\n",
    "log.info(\"INPUT_HEIGHT:\", INPUT_HEIGHT)\n",
    "log.info(\"CHECKPOINT_PATH:\", CHECKPOINT_PATH)\n",
    "log.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, files, keypoints, augmentation=True, padding=30):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.keypoints = keypoints\n",
    "        self.padding = padding\n",
    "\n",
    "        T = []\n",
    "        # T.append(A.Crop(0, 28, 1920, 1080 - 28))  # 1920x1080 --> 1920x1024\n",
    "        # T.append(A.Resize(512, 1024))\n",
    "        if augmentation:\n",
    "            T.append(A.ImageCompression())\n",
    "            # T.append(A.ShiftScaleRotate(border_mode=cv2.BORDER_CONSTANT, value=0, rotate_limit=0))\n",
    "            # T.append(utils.HorizontalFlipEx())\n",
    "            T.append(A.Cutout())\n",
    "            T_ = []\n",
    "            T_.append(A.RandomBrightnessContrast())\n",
    "            T_.append(A.RandomGamma())\n",
    "            T_.append(A.RandomBrightness())\n",
    "            T_.append(A.RandomContrast())\n",
    "            T.append(A.OneOf(T_))\n",
    "            T.append(A.GaussNoise())\n",
    "            T.append(A.Blur())\n",
    "        T.append(A.Normalize())\n",
    "        T.append(ToTensorV2())\n",
    "\n",
    "        self.transform = A.Compose(\n",
    "            transforms=T,\n",
    "            bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"]),\n",
    "            keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n",
    "            # TODO 영역을 벗어난 keypoint는 그 영역의 한도 값으로 설정해줄 것?\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = str(self.files[idx])\n",
    "        image = imageio.imread(file)\n",
    "\n",
    "        keypoint = self.keypoints[idx]\n",
    "        box = utils.keypoint2box(keypoint, self.padding)\n",
    "        box = np.expand_dims(box, 0)\n",
    "        labels = np.array([0], dtype=np.int64)\n",
    "        a = self.transform(image=image, labels=labels, bboxes=box, keypoints=keypoint)\n",
    "\n",
    "        image = a[\"image\"]\n",
    "        bbox = list(map(int, a[\"bboxes\"][0]))\n",
    "        keypoint = torch.tensor(a[\"keypoints\"], dtype=torch.float32)\n",
    "        image, keypoint, heatmap, ratio = self._resize_image(image, bbox, keypoint)\n",
    "\n",
    "        return file, image, keypoint, heatmap, ratio\n",
    "\n",
    "    def _resize_image(self, image, bbox, keypoint):\n",
    "        # efficientdet에서 찾은 범위만큼 이미지를 자름\n",
    "        image = image[:, bbox[1] : bbox[3], bbox[0] : bbox[2]]\n",
    "\n",
    "        # HRNet의 입력 이미지 크기로 resize\n",
    "        ratio = torch.tensor((INPUT_WIDTH / image.shape[2], INPUT_HEIGHT / image.shape[1]), dtype=torch.float32)\n",
    "        image = F.interpolate(image.unsqueeze(0), (INPUT_HEIGHT, INPUT_WIDTH))[0]\n",
    "\n",
    "        # bbox만큼 빼줌\n",
    "        keypoint[:, 0] -= bbox[0]\n",
    "        keypoint[:, 1] -= bbox[1]\n",
    "\n",
    "        # 이미지를 resize해준 비율만큼 곱해줌\n",
    "        keypoint[:, 0] *= ratio[0]\n",
    "        keypoint[:, 1] *= ratio[1]\n",
    "        # TODO: 잘못된 keypoint가 있으면 고쳐줌\n",
    "\n",
    "        # HRNet은 1/4로 resize된 출력이 나오므로 4로 나눠줌\n",
    "        keypoint /= 4\n",
    "\n",
    "        # keypoint를 heatmap으로 변환\n",
    "        # TODO: 완전히 정답이 아니면 틀린 것과 같은 점수. 좀 부드럽게 만들 수는 없을지?\n",
    "        # heatmap regression loss중에 soft~~~ 한 이름이 있던거같은데\n",
    "        heatmap = utils.keypoints2heatmaps(keypoint, INPUT_HEIGHT // 4, INPUT_WIDTH // 4)\n",
    "\n",
    "        return image, keypoint, heatmap, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestKeypointDataset(Dataset):\n",
    "    def __init__(self, files, offsets, ratios, augmentation=False):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.offsets = offsets\n",
    "        self.ratios = ratios\n",
    "\n",
    "        T = []\n",
    "        if augmentation:\n",
    "            T.append(A.ImageCompression())\n",
    "            T.append(A.Cutout())\n",
    "            T_ = []\n",
    "            T_.append(A.RandomBrightnessContrast())\n",
    "            T_.append(A.RandomGamma())\n",
    "            T_.append(A.RandomBrightness())\n",
    "            T_.append(A.RandomContrast())\n",
    "            T.append(A.OneOf(T_))\n",
    "            T.append(A.GaussNoise())\n",
    "            T.append(A.Blur())\n",
    "        T.append(A.Normalize())\n",
    "        T.append(ToTensorV2())\n",
    "\n",
    "        self.transform = A.Compose(transforms=T)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = str(self.files[idx])\n",
    "        offset = torch.tensor(self.offsets[idx], dtype=torch.float32)\n",
    "        ratio = torch.tensor(self.ratios[idx], dtype=torch.float32)\n",
    "\n",
    "        image = imageio.imread(file)\n",
    "        a = self.transform(image=image)\n",
    "        image = a[\"image\"]\n",
    "\n",
    "        return file, image, offset, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs = np.array(sorted(list((DATA_DIR / \"train_imgs\").glob(\"*.jpg\"))))\n",
    "df = pd.read_csv(\"data/ori/train_df.csv\")\n",
    "total_keypoints = df.to_numpy()[:, 1:].astype(np.float32)\n",
    "total_keypoints = np.stack([total_keypoints[:, 0::2], total_keypoints[:, 1::2]], axis=2)\n",
    "\n",
    "# 오류가 있는 데이터는 학습에서 제외\n",
    "total_imgs_, total_keypoints_ = [], []\n",
    "for i in range(len(total_imgs)):\n",
    "    if i not in error_list:\n",
    "        total_imgs_.append(total_imgs[i])\n",
    "        total_keypoints_.append(total_keypoints[i])\n",
    "total_imgs = np.array(total_imgs_)\n",
    "total_keypoints = np.array(total_keypoints_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = np.array(sorted(list((DATA_DIR / \"test_imgs\").glob(\"*.jpg\"))))\n",
    "\n",
    "with open(\"data/test_imgs_effdet/data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    offsets = data[\"offset\"]\n",
    "    ratios = data[\"ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointMSELoss(nn.Module):\n",
    "    def __init__(self, use_target_weight=False):\n",
    "        super(JointMSELoss, self).__init__()\n",
    "        self.criterion = nn.MSELoss(reduction=\"mean\")\n",
    "        self.use_target_weight = use_target_weight\n",
    "\n",
    "    def forward(self, output, target, target_weight=None):\n",
    "        batch_size = output.size(0)\n",
    "        num_joints = output.size(1)\n",
    "        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)\n",
    "        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\n",
    "        loss = 0\n",
    "\n",
    "        for idx in range(num_joints):\n",
    "            heatmap_pred = heatmaps_pred[idx].squeeze()\n",
    "            heatmap_gt = heatmaps_gt[idx].squeeze()\n",
    "            if self.use_target_weight:\n",
    "                loss += 0.5 * self.criterion(heatmap_pred.mul(target_weight[:, idx]), heatmap_gt.mul(target_weight[:, idx]))\n",
    "            else:\n",
    "                loss += 0.5 * self.criterion(heatmap_pred, heatmap_gt)\n",
    "\n",
    "        return loss / num_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointLoss(nn.Module):\n",
    "    def __init__(self, joint=False, use_target_weight=False):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.MSELoss(reduction=\"mean\")\n",
    "        self.joint = joint\n",
    "        self.use_target_weight = use_target_weight\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = x.flatten(2).flatten(0, 1)\n",
    "        y = y.flatten(2).flatten(0, 1).argmax(1)\n",
    "        loss1 = F.cross_entropy(x, y)\n",
    "\n",
    "        if self.joint:\n",
    "            loss2 = self.joint_mse_loss(x, y)\n",
    "            return loss1 + loss2\n",
    "        else:\n",
    "            return loss1\n",
    "\n",
    "    def joint_mse_loss(self, output, target, target_weight=None):\n",
    "        batch_size = output.size(0)\n",
    "        num_joints = output.size(1)\n",
    "        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)\n",
    "        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\n",
    "        loss = 0\n",
    "\n",
    "        for idx in range(num_joints):\n",
    "            heatmap_pred = heatmaps_pred[idx].squeeze()\n",
    "            heatmap_gt = heatmaps_gt[idx].squeeze()\n",
    "            if self.use_target_weight:\n",
    "                loss += 0.5 * self.criterion(heatmap_pred.mul(target_weight[:, idx]), heatmap_gt.mul(target_weight[:, idx]))\n",
    "            else:\n",
    "                loss += 0.5 * self.criterion(heatmap_pred, heatmap_gt)\n",
    "\n",
    "        return loss / num_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointRMSE(nn.Module):\n",
    "    @torch.no_grad()\n",
    "    def forward(self, pred_heatmaps: torch.Tensor, real_heatmaps: torch.Tensor, ratios: torch.Tensor):\n",
    "        W = pred_heatmaps.size(3)\n",
    "        pred_positions = pred_heatmaps.flatten(2).argmax(2)\n",
    "        real_positions = real_heatmaps.flatten(2).argmax(2)\n",
    "        pred_positions = torch.stack((pred_positions // W, pred_positions % W), 2).type(torch.float32)\n",
    "        real_positions = torch.stack((real_positions // W, real_positions % W), 2).type(torch.float32)\n",
    "        # print(pred_positions.shape, real_positions.shape, ratios.shape)\n",
    "        pred_positions = pred_positions * 4 / ratios.unsqueeze(1)  # position: (B, 24, 2), ratio: (B, 2)\n",
    "        real_positions = real_positions * 4 / ratios.unsqueeze(1)\n",
    "        loss = (pred_positions - real_positions).square().mean().sqrt()\n",
    "\n",
    "        \"\"\"\n",
    "        W = x.size(3)\n",
    "        xp = x.flatten(2).argmax(2)\n",
    "        xx = (xp % W) / ratios[:, 0:1] * 4\n",
    "        xy = (xp // W) / ratios[:, 1:2] * 4\n",
    "        yp = y.flatten(2).argmax(2)\n",
    "        yx = (yp % W) / ratios[:, 0:1] * 4\n",
    "        yy = (yp // W) / ratios[:, 1:2] * 4\n",
    "\n",
    "        diff = ((xx - yx) ** 2 + (xy - yy) ** 2) / 2\n",
    "        loss = diff.mean().sqrt()\n",
    "        \"\"\"\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainOutputBean:\n",
    "    loss = utils.AverageMeter()\n",
    "    rmse = utils.AverageMeter()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.loss = self.loss()\n",
    "        self.rmse = self.rmse()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainOutputBean:\n",
    "    loss = utils.AverageMeter()\n",
    "    rmse = utils.AverageMeter()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.loss = self.loss()\n",
    "        self.rmse = self.rmse()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 모델, 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HRNet 생성\n",
    "if POSE_MODEL == \"HRNet-W32\":\n",
    "    width = 32\n",
    "elif POSE_MODEL == \"HRNet-W48\":\n",
    "    width = 48\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "pose_model = networks.PoseHighResolutionNet(width)\n",
    "pose_model.load_state_dict(torch.load(f\"networks/models/pose_hrnet_w{width}_384x288.pth\"))\n",
    "\n",
    "final_layer = nn.Conv2d(width, 24, 1)\n",
    "with torch.no_grad():\n",
    "    final_layer.weight[:17] = pose_model.final_layer.weight\n",
    "    final_layer.bias[:17] = pose_model.final_layer.bias\n",
    "    pose_model.final_layer = final_layer\n",
    "pose_model.cuda()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = KeypointLoss().cuda()\n",
    "criterion_rmse = KeypointRMSE().cuda()\n",
    "\n",
    "if SAM:\n",
    "    optimizer = utils.SAM(pose_model.parameters(), optim.AdamW, lr=LR)\n",
    "else:\n",
    "    optimizer = optim.AdamW(pose_model.parameters(), lr=LR)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=4, verbose=True)\n",
    "\n",
    "# 기타\n",
    "best_loss = math.inf\n",
    "earlystop_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(path):\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model\": pose_model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"best_loss\": best_loss,\n",
    "            \"earlystop_cnt\": earlystop_cnt,\n",
    "        },\n",
    "        path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    print(\"Load pretrained\", path)\n",
    "    ckpt = torch.load(path)\n",
    "    pose_model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    epoch = ckpt[\"epoch\"]\n",
    "    best_loss = ckpt[\"best_loss\"]\n",
    "    earlystop_cnt = ckpt[\"earlystop_cnt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "indices = list(kf.split(total_imgs))\n",
    "\n",
    "# train dataset\n",
    "train_idx, valid_idx = indices[0]\n",
    "ds_train = KeypointDataset(\n",
    "    total_imgs[train_idx],\n",
    "    total_keypoints[train_idx],\n",
    "    augmentation=True,\n",
    "    padding=PADDING,\n",
    ")\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n",
    "\n",
    "# validation dataset\n",
    "ds_valid = KeypointDataset(\n",
    "    total_imgs[valid_idx],\n",
    "    total_keypoints[valid_idx],\n",
    "    augmentation=False,\n",
    "    padding=PADDING,\n",
    ")\n",
    "dl_valid = DataLoader(ds_valid, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)\n",
    "\n",
    "# test dataset\n",
    "ds_test = TestKeypointDataset(test_imgs, offsets, ratios, augmentation=False)\n",
    "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECKPOINT_PATH is not None:\n",
    "    load(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    torch.cuda.empty_cache()\n",
    "    pose_model.train()\n",
    "\n",
    "    O = TrainOutputBean()\n",
    "    with tqdm(total=len(dl_train.dataset), ncols=100, leave=False, file=sys.stdout, desc=f\"Train {epoch:03d}\") as t:\n",
    "        for files, imgs, keypoints, target_heatmaps, ratios in dl_train:\n",
    "            imgs_, target_heatmaps_ = imgs.cuda(), target_heatmaps.cuda()\n",
    "            pred_heatmaps_ = pose_model(imgs_)\n",
    "            loss = criterion(pred_heatmaps_, target_heatmaps_)\n",
    "            rmse = criterion_rmse(pred_heatmaps_, target_heatmaps_, ratios.cuda())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if isinstance(optimizer, utils.SAM):\n",
    "                optimizer.first_step()\n",
    "                criterion(pose_model(imgs_), target_heatmaps_).backward()\n",
    "                optimizer.second_step()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            O.loss.update(loss.item())\n",
    "            O.rmse.update(rmse.item())\n",
    "            t.set_postfix_str(f\"loss: {loss.item():.6f}, rmse: {rmse.item():.6f}\", refresh=False)\n",
    "            t.update(len(imgs))\n",
    "\n",
    "    return O.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_loop():\n",
    "    torch.cuda.empty_cache()\n",
    "    pose_model.eval()\n",
    "\n",
    "    O = TrainOutputBean()\n",
    "    with tqdm(total=len(dl_valid.dataset), ncols=100, leave=False, file=sys.stdout, desc=f\"Valid {epoch:03d}\") as t:\n",
    "        for files, imgs, keypoints, target_heatmaps, ratios in dl_valid:\n",
    "            imgs_, target_heatmaps_ = imgs.cuda(), target_heatmaps.cuda()\n",
    "            pred_heatmaps_ = pose_model(imgs_)\n",
    "            loss = criterion(pred_heatmaps_, target_heatmaps_)\n",
    "            rmse = criterion_rmse(pred_heatmaps_, target_heatmaps_, ratios.cuda())\n",
    "\n",
    "            O.loss.update(loss.item())\n",
    "            O.rmse.update(rmse.item())\n",
    "            t.set_postfix_str(f\"loss: {loss.item():.6f}, rmse: {rmse.item():.6f}\", refresh=False)\n",
    "            t.update(len(imgs))\n",
    "\n",
    "    return O.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2021-03-21 21:38:28  INFO] Finetune Step 1\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:43:13  INFO] Epoch: 001, loss: 7.630778;7.264316, rmse 113.307251;107.709692\u001b[0m\n",
      "\u001b[34m[2021-03-21 21:47:55  INFO] Epoch: 002, loss: 6.295161;6.115364, rmse 90.554254;86.004646\u001b[0m  \n",
      "\u001b[34m[2021-03-21 22:06:44  INFO] Epoch: 006, loss: 4.435437;4.403634, rmse 44.208360;43.261186\u001b[0m  \n",
      "\u001b[34m[2021-03-21 22:11:26  INFO] Epoch: 007, loss: 4.218349;4.195462, rmse 39.741538;39.042996\u001b[0m  \n",
      "Train[008]:  89%|█████████████▎ | 2900/3275 [03:51<00:29, 12.63it/s, loss: 2.576661, rmse: 7.768835]"
     ]
    }
   ],
   "source": [
    "log.info(\"Finetune Step 1\")\n",
    "pose_model.freeze_head()\n",
    "for epoch in range(START_EPOCH, STEP1_EPOCHS + 1):\n",
    "    to = train_loop()\n",
    "    vo = valid_loop()\n",
    "\n",
    "    log.info(f\"Epoch: {epoch:03d}, loss: {to.loss:.6f};{vo.loss:.6f}, rmse {to.rmse:.6f};{vo.rmse:.6f}\")\n",
    "\n",
    "    if best_loss > vo.loss:\n",
    "        best_loss = vo.loss\n",
    "        save(RESULT_DIR / f\"ckpt-{UID}_{FOLD}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Finetune Step 2\")\n",
    "pose_model.freeze_tail()\n",
    "for epoch in range(epoch, STEP2_EPOCHS + 1):\n",
    "    to = train_loop()\n",
    "    vo = valid_loop()\n",
    "\n",
    "    log.info(f\"Epoch: {epoch:03d}, loss: {to.loss:.6f};{vo.loss:.6f}, rmse {to.rmse:.6f};{vo.rmse:.6f}\")\n",
    "\n",
    "    if best_loss > vo.loss:\n",
    "        best_loss = vo.loss\n",
    "        save(RESULT_DIR / f\"ckpt-{UID}_{FOLD}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Finetune Step 3\")\n",
    "pose_model.unfreeze_all()\n",
    "for epoch in range(epoch, STEP3_EPOCHS + 1):\n",
    "    to = train_loop()\n",
    "    vo = valid_loop()\n",
    "\n",
    "    log.info(f\"Epoch: {epoch:03d}, loss: {to.loss:.6f};{vo.loss:.6f}, rmse {to.rmse:.6f};{vo.rmse:.6f}\")\n",
    "    scheduler.step(vo.loss)\n",
    "\n",
    "    if best_loss > vo.loss:\n",
    "        best_loss = vo.loss\n",
    "        earlystop_cnt = 0\n",
    "        save(RESULT_DIR / f\"ckpt-{UID}_{FOLD}.pth\")\n",
    "    elif earlystop_cnt >= 10:\n",
    "        log.info(f\"Stop training at epoch\", epoch)\n",
    "        break\n",
    "    else:\n",
    "        earlystop_cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
