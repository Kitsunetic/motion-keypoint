{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Sequence, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networks\n",
    "import utils\n",
    "from error_list import error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_MODEL = \"HRNet-W48\"\n",
    "DET_PRETRAINED = \"\"\n",
    "RESULT_DIR = Path(\"results/hrnet+det\")\n",
    "\n",
    "LR = 1e-4  # transfer learning이니깐 좀 작게 주는게 좋을 것 같아서 1e-4\n",
    "BATCH_SIZE = 40\n",
    "START_EPOCH = 1\n",
    "SAM = True\n",
    "FOLDS = [1, 2, 3, 4, 5]\n",
    "PADDING = 30\n",
    "\n",
    "n = datetime.now()\n",
    "UID = f\"{n.year:04d}{n.month:02d}{n.day:02d}-{n.hour:02d}{n.minute:02d}{n.second:02d}\"\n",
    "SEED = 20210309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fa23e18a910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs = np.array(sorted(list(Path(\"data/ori/train_imgs/\").glob(\"*.jpg\"))))\n",
    "test_imgs = np.array(sorted(list(Path(\"data/ori/test_imgs/\").glob(\"*.jpg\"))))\n",
    "\n",
    "df = pd.read_csv(\"data/ori/train_df.csv\")\n",
    "total_keypoints = df.to_numpy()[:, 1:].astype(np.float32)\n",
    "total_keypoints = np.stack([total_keypoints[:, 0::2], total_keypoints[:, 1::2]], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs_, total_keypoints_ = [], []\n",
    "for i in range(len(total_imgs)):\n",
    "    if i not in error_list:\n",
    "        total_imgs_.append(total_imgs[i])\n",
    "        total_keypoints_.append(total_keypoints[i])\n",
    "total_imgs = np.array(total_imgs_)\n",
    "total_keypoints = np.array(total_keypoints_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, files, augmentation=False):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "\n",
    "        T = []\n",
    "        if augmentation:\n",
    "            T.append(A.ImageCompression())\n",
    "            T.append(A.ShiftScaleRotate(border_mode=cv2.BORDER_CONSTANT, value=0, rotate_limit=0))\n",
    "            T.append(A.Cutout())\n",
    "            T_ = []\n",
    "            T_.append(A.RandomBrightnessContrast())\n",
    "            T_.append(A.RandomGamma())\n",
    "            T_.append(A.RandomBrightness())\n",
    "            T_.append(A.RandomContrast())\n",
    "            T.append(A.OneOf(T_))\n",
    "            T.append(A.GaussNoise())\n",
    "            T.append(A.Blur())\n",
    "        T.append(A.Normalize())\n",
    "        T.append(ToTensorV2())\n",
    "\n",
    "        self.transform = A.Compose(transforms=T)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = str(self.files[idx])\n",
    "        image = imageio.imread(file)\n",
    "        a = self.transform(image=image)\n",
    "        image = a[\"image\"]\n",
    "\n",
    "        return file, image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainInputBean:\n",
    "    def __init__(self):\n",
    "        # HRNet 생성\n",
    "        if POSE_MODEL == \"HRNet-W32\":\n",
    "            width = 32\n",
    "        elif POSE_MODEL == \"HRNet-W48\":\n",
    "            width = 48\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        self.pose_model = networks.PoseHighResolutionNet(width)\n",
    "        self.pose_model.load_state_dict(torch.load(f\"networks/models/pose_hrnet_w{width}_384x288.pth\"))\n",
    "\n",
    "        final_layer = nn.Conv2d(width, 24, 1)\n",
    "        with torch.no_grad():\n",
    "            final_layer.weight[:17] = self.pose_model.final_layer.weight\n",
    "            final_layer.bias[:17] = self.pose_model.final_layer.bias\n",
    "            self.pose_model.final_layer = final_layer\n",
    "        self.pose_model.cuda()\n",
    "\n",
    "        # Criterion / Optimizer\n",
    "        # self.criterion = JointMSELoss().cuda()\n",
    "        self.criterion = KeypointLoss().cuda()\n",
    "        self.criterion_rmse = KeypointRMSE().cuda()\n",
    "        if SAM:\n",
    "            self.optimizer = utils.SAM(self.pose_model.parameters(), optim.AdamW, lr=LR)\n",
    "        else:\n",
    "            self.optimizer = optim.AdamW(self.pose_model.parameters(), lr=LR)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, factor=0.5, patience=4, verbose=True)\n",
    "\n",
    "        # 기타\n",
    "        self.epoch = START_EPOCH\n",
    "        self.best_loss = math.inf\n",
    "        self.earlystop_cnt = 0\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": self.pose_model.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "                \"epoch\": self.epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"earlystop_cnt\": self.earlystop_cnt,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load(self, path):\n",
    "        print(\"Load pretrained\", path)\n",
    "        ckpt = torch.load(path)\n",
    "        self.pose_model.load_state_dict(ckpt[\"model\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        try:\n",
    "            self.epoch = ckpt[\"epoch\"]\n",
    "            self.best_loss = ckpt[\"best_loss\"]\n",
    "            self.earlystop_cnt = ckpt[\"earlystop_cnt\"]\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = KeypointDataset(test_imgs, augmentation=False)\n",
    "dl_test = DataLoader(ds_test, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = TrainInputBean()\n",
    "B.load('results/')\n",
    "B.pose_model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained results/hrnet+det/ckpt-20210320-005140_1.pth\n"
     ]
    }
   ],
   "source": [
    "B.load(\"results/hrnet+det/ckpt-20210320-005140_1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation 1epoch 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(\"results/hrnet+det-example\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid[199]: 100%|████████████████| 819/819 [00:15<00:00, 52.70it/s, loss: 2.321360, rmse: 12.936345]\n"
     ]
    }
   ],
   "source": [
    "vo = valid_loop(B, dl_valid, OUTPUT_DIR / \"valid_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test는 effdet이 필요함..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
