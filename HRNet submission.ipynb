{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Sequence, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networks\n",
    "import utils\n",
    "from error_list import error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_MODEL = \"HRNet-W48\"\n",
    "DET_PRETRAINED = \"\"\n",
    "RESULT_DIR = Path(\"results/hrnet+det\")\n",
    "\n",
    "LR = 1e-4  # transfer learning이니깐 좀 작게 주는게 좋을 것 같아서 1e-4\n",
    "BATCH_SIZE = 40\n",
    "START_EPOCH = 1\n",
    "SAM = True\n",
    "FOLDS = [1, 2, 3, 4, 5]\n",
    "PADDING = 30\n",
    "\n",
    "n = datetime.now()\n",
    "UID = f\"{n.year:04d}{n.month:02d}{n.day:02d}-{n.hour:02d}{n.minute:02d}{n.second:02d}\"\n",
    "SEED = 20210309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs = np.array(sorted(list(Path(\"data/ori/train_imgs/\").glob(\"*.jpg\"))))\n",
    "test_imgs = np.array(sorted(list(Path(\"data/ori/test_imgs/\").glob(\"*.jpg\"))))\n",
    "\n",
    "df = pd.read_csv(\"data/ori/train_df.csv\")\n",
    "total_keypoints = df.to_numpy()[:, 1:].astype(np.float32)\n",
    "total_keypoints = np.stack([total_keypoints[:, 0::2], total_keypoints[:, 1::2]], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs_, total_keypoints_ = [], []\n",
    "for i in range(len(total_imgs)):\n",
    "    if i not in error_list:\n",
    "        total_imgs_.append(total_imgs[i])\n",
    "        total_keypoints_.append(total_keypoints[i])\n",
    "total_imgs = np.array(total_imgs_)\n",
    "total_keypoints = np.array(total_keypoints_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointDataset(Dataset):\n",
    "    def __init__(self, files, keypoints, augmentation=True, padding=30):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.keypoints = keypoints\n",
    "        self.padding = padding\n",
    "\n",
    "        T = []\n",
    "        # T.append(A.Crop(0, 28, 1920, 1080 - 28))  # 1920x1080 --> 1920x1024\n",
    "        # T.append(A.Resize(512, 1024))\n",
    "        if augmentation:\n",
    "            T.append(A.ImageCompression())\n",
    "            T.append(A.ShiftScaleRotate(border_mode=cv2.BORDER_CONSTANT, value=0, rotate_limit=0))\n",
    "            T.append(utils.HorizontalFlipEx())\n",
    "            T.append(A.Cutout())\n",
    "            T_ = []\n",
    "            T_.append(A.RandomBrightnessContrast())\n",
    "            T_.append(A.RandomGamma())\n",
    "            T_.append(A.RandomBrightness())\n",
    "            T_.append(A.RandomContrast())\n",
    "            T.append(A.OneOf(T_))\n",
    "            T.append(A.GaussNoise())\n",
    "            T.append(A.Blur())\n",
    "        T.append(A.Normalize())\n",
    "        T.append(ToTensorV2())\n",
    "\n",
    "        self.transform = A.Compose(\n",
    "            transforms=T,\n",
    "            bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"]),\n",
    "            keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n",
    "            # TODO 영역을 벗어난 keypoint는 그 영역의 한도 값으로 설정해줄 것?\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = imageio.imread(self.files[idx])\n",
    "\n",
    "        keypoint = self.keypoints[idx]\n",
    "        box = utils.keypoint2box(keypoint, self.padding)\n",
    "        box = np.expand_dims(box, 0)\n",
    "        labels = np.array([0], dtype=np.int64)\n",
    "        a = self.transform(image=image, labels=labels, bboxes=box, keypoints=keypoint)\n",
    "\n",
    "        image = a[\"image\"]\n",
    "        bbox = list(map(int, a[\"bboxes\"][0]))\n",
    "        keypoint = torch.tensor(a[\"keypoints\"], dtype=torch.float32)\n",
    "        image, keypoint, heatmap, ratio = self._resize_image(image, bbox, keypoint)\n",
    "\n",
    "        return image, keypoint, heatmap, ratio\n",
    "\n",
    "    def _resize_image(self, image, bbox, keypoint):\n",
    "        # efficientdet에서 찾은 범위만큼 이미지를 자름\n",
    "        image = image[:, bbox[1] : bbox[3], bbox[0] : bbox[2]]\n",
    "\n",
    "        # HRNet의 입력 이미지 크기로 resize\n",
    "        ratio = torch.tensor((288 / image.shape[2], 384 / image.shape[1]), dtype=torch.float32)\n",
    "        image = F.interpolate(image.unsqueeze(0), (384, 288))[0]\n",
    "\n",
    "        # bbox만큼 빼줌\n",
    "        keypoint[:, 0] -= bbox[0]\n",
    "        keypoint[:, 1] -= bbox[1]\n",
    "\n",
    "        # 이미지를 resize해준 비율만큼 곱해줌\n",
    "        keypoint[:, 0] *= ratio[0]\n",
    "        keypoint[:, 1] *= ratio[1]\n",
    "        # TODO: 잘못된 keypoint가 있으면 고쳐줌\n",
    "\n",
    "        # HRNet은 1/4로 resize된 출력이 나오므로 4로 나눠줌\n",
    "        keypoint /= 4\n",
    "\n",
    "        # keypoint를 heatmap으로 변환\n",
    "        # TODO: 완전히 정답이 아니면 틀린 것과 같은 점수. 좀 부드럽게 만들 수는 없을지?\n",
    "        # heatmap regression loss중에 soft~~~ 한 이름이 있던거같은데\n",
    "        heatmap = utils.keypoints2heatmaps(keypoint, 96, 72)\n",
    "\n",
    "        return image, keypoint, heatmap, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "indices = list(kf.split(total_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = KeypointDataset(total_imgs[train_idx], total_keypoints[train_idx], augmentation=True, padding=PADDING)\n",
    "ds_valid = KeypointDataset(total_imgs[valid_idx], total_keypoints[valid_idx], augmentation=False, padding=PADDING)\n",
    "dl_train = DataLoader(ds_train, batch_size=2, num_workers=4, shuffle=True)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=2, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointMSELoss(nn.Module):\n",
    "    \"\"\"https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/lib/core/loss.py\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor):\n",
    "        num_joints = pred.size(1)\n",
    "        mse = (pred - target).square().mean((2, 3)).sum()\n",
    "        loss = mse / num_joints\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointRMSE(nn.Module):\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, y, ratios):\n",
    "        W = x.size(3)\n",
    "        xp = x.flatten(2).argmax(2)\n",
    "        xx = (xp % W) / ratios[:, 0:1] * 4\n",
    "        xy = (xp // W) / ratios[:, 1:2] * 4\n",
    "        yp = y.flatten(2).argmax(2)\n",
    "        yx = (yp % W) / ratios[:, 0:1] * 4\n",
    "        yy = (yp // W) / ratios[:, 1:2] * 4\n",
    "\n",
    "        if random.random() <= 0.01:\n",
    "            keypoints = torch.stack([xx, xy, yx, yy], 2)\n",
    "            print(keypoints[0])\n",
    "\n",
    "        diff = ((xx - yx) ** 2 + (xy - yy) ** 2) / 2\n",
    "        loss = diff.mean().sqrt()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainOutputBean:\n",
    "    loss = utils.AverageMeter()\n",
    "    rmse = utils.AverageMeter()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.loss = self.loss()\n",
    "        self.rmse = self.rmse()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainInputBean:\n",
    "    def __init__(self):\n",
    "        # HRNet 생성\n",
    "        if POSE_MODEL == \"HRNet-W32\":\n",
    "            width = 32\n",
    "        elif POSE_MODEL == \"HRNet-W48\":\n",
    "            width = 48\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        self.pose_model = networks.PoseHighResolutionNet(width)\n",
    "        self.pose_model.load_state_dict(torch.load(f\"networks/models/pose_hrnet_w{width}_384x288.pth\"))\n",
    "\n",
    "        final_layer = nn.Conv2d(width, 24, 1)\n",
    "        with torch.no_grad():\n",
    "            final_layer.weight[:17] = self.pose_model.final_layer.weight\n",
    "            final_layer.bias[:17] = self.pose_model.final_layer.bias\n",
    "            self.pose_model.final_layer = final_layer\n",
    "        self.pose_model.cuda()\n",
    "\n",
    "        # Criterion / Optimizer\n",
    "        # self.criterion = JointMSELoss().cuda()\n",
    "        self.criterion = KeypointLoss().cuda()\n",
    "        self.criterion_rmse = KeypointRMSE().cuda()\n",
    "        if SAM:\n",
    "            self.optimizer = utils.SAM(self.pose_model.parameters(), optim.AdamW, lr=LR)\n",
    "        else:\n",
    "            self.optimizer = optim.AdamW(self.pose_model.parameters(), lr=LR)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, factor=0.5, patience=4, verbose=True)\n",
    "\n",
    "        # 기타\n",
    "        self.epoch = START_EPOCH\n",
    "        self.best_loss = math.inf\n",
    "        self.earlystop_cnt = 0\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": self.pose_model.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "                \"epoch\": self.epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"earlystop_cnt\": self.earlystop_cnt,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load(self, path):\n",
    "        print(\"Load pretrained\", path)\n",
    "        ckpt = torch.load(path)\n",
    "        self.pose_model.load_state_dict(ckpt[\"model\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        self.epoch = ckpt[\"epoch\"]\n",
    "        self.best_loss = ckpt[\"best_loss\"]\n",
    "        self.earlystop_cnt = ckpt[\"earlystop_cnt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_loop(B: TrainInputBean, dl: DataLoader):\n",
    "    torch.cuda.empty_cache()\n",
    "    B.pose_model.eval()\n",
    "\n",
    "    O = TrainOutputBean()\n",
    "    with tqdm(total=len(dl.dataset), ncols=100, leave=False, file=sys.stdout, desc=f\"Valid[{B.epoch:03d}]\") as t:\n",
    "        for imgs, keypoints, target_heatmaps, ratios in dl:\n",
    "            imgs_, target_heatmaps_ = imgs.cuda(), target_heatmaps.cuda()\n",
    "            pred_heatmaps_ = B.pose_model(imgs_)\n",
    "            loss = B.criterion(pred_heatmaps_, target_heatmaps_)\n",
    "            rmse = B.criterion_rmse(pred_heatmaps_, target_heatmaps_, ratios.cuda())\n",
    "\n",
    "            O.loss.update(loss.item())\n",
    "            O.rmse.update(rmse.item())\n",
    "            t.set_postfix_str(f\"loss: {loss.item():.6f}, rmse: {rmse.item():.6f}\", refresh=False)\n",
    "            t.update(len(imgs))\n",
    "\n",
    "    return O.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width = 48\n",
    "pose_model1 = networks.PoseHighResolutionNet(width)\n",
    "# pose_model1.load_state_dict(torch.load(f\"networks/models/pose_hrnet_w{width}_384x288.pth\"))\n",
    "final_layer = nn.Conv2d(width, 24, 1)\n",
    "with torch.no_grad():\n",
    "    final_layer.weight[:17] = pose_model1.final_layer.weight\n",
    "    final_layer.bias[:17] = pose_model1.final_layer.bias\n",
    "    pose_model1.final_layer = final_layer\n",
    "pose_model1.load_state_dict(torch.load(f\"results/hrnet+det/ckpt-20210319-130005_1.pth\", map_location=\"cpu\")[\"model\"])\n",
    "\n",
    "pose_model2 = networks.PoseHighResolutionNet(width)\n",
    "final_layer = nn.Conv2d(width, 24, 1)\n",
    "with torch.no_grad():\n",
    "    final_layer.weight[:17] = pose_model2.final_layer.weight\n",
    "    final_layer.bias[:17] = pose_model2.final_layer.bias\n",
    "    pose_model2.final_layer = final_layer\n",
    "pose_model2.load_state_dict(torch.load(f\"results/hrnet+det/ckpt-20210320-005140_1.pth\", map_location=\"cpu\")[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
