{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thorough-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-upper",
   "metadata": {},
   "source": [
    "처음에는 ground truth keypoint를 기반으로 얻은 가상의 bounding box로 detection model을 학습시켰지만,\n",
    "가상의 bounding box가 정확도가 떨어지기 때문에 모델이 오히려 잘못된 feature를 학습하고 성능이 떨어지는 경우를 봤습니다.\n",
    "하지만 한편으로는 가상의 bounding box로 학습을 한 경우가 더 잘 탐지하는 경우도 있기 때문에(특히 운동기구 등으로 몸이 일부가 가려진 경우) 둘 모두를 사용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "prospective-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "from pprint import pformat\n",
    "from typing import Iterable, List\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import yaml\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from easydict import EasyDict\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from effdet_torch_singlefile import EfficientDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "flying-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최정명님이 공유해주신 잘못된 데이터들\n",
    "error_list = [\n",
    "    317,\n",
    "    869,\n",
    "    873,\n",
    "    877,\n",
    "    911,\n",
    "    1559,\n",
    "    1560,\n",
    "    1562,\n",
    "    1566,\n",
    "    1575,\n",
    "    1577,\n",
    "    1578,\n",
    "    1582,\n",
    "    1606,\n",
    "    1607,\n",
    "    1622,\n",
    "    1623,\n",
    "    1624,\n",
    "    1625,\n",
    "    1629,\n",
    "    3968,\n",
    "    4115,\n",
    "    4116,\n",
    "    4117,\n",
    "    4118,\n",
    "    4119,\n",
    "    4120,\n",
    "    4121,\n",
    "    4122,\n",
    "    4123,\n",
    "    4124,\n",
    "    4125,\n",
    "    4126,\n",
    "    4127,\n",
    "    4128,\n",
    "    4129,\n",
    "    4130,\n",
    "    4131,\n",
    "    4132,\n",
    "    4133,\n",
    "    4134,\n",
    "    4135,\n",
    "    4136,\n",
    "    4137,\n",
    "    4138,\n",
    "    4139,\n",
    "    4140,\n",
    "    4141,\n",
    "    4142,\n",
    "    4143,\n",
    "    4144,\n",
    "    4145,\n",
    "    4146,\n",
    "    4147,\n",
    "    4148,\n",
    "    4149,\n",
    "    4150,\n",
    "    4151,\n",
    "    4152,\n",
    "    4153,\n",
    "    4154,\n",
    "    4155,\n",
    "    4156,\n",
    "    4157,\n",
    "    4158,\n",
    "    4159,\n",
    "    4160,\n",
    "    4161,\n",
    "    4162,\n",
    "    4163,\n",
    "    4164,\n",
    "    4165,\n",
    "    4166,\n",
    "    4167,\n",
    "    4168,\n",
    "    4169,\n",
    "    4170,\n",
    "    4171,\n",
    "    4172,\n",
    "    4173,\n",
    "    4174,\n",
    "    4175,\n",
    "    4176,\n",
    "    4177,\n",
    "    4178,\n",
    "    4179,\n",
    "    4180,\n",
    "    4181,\n",
    "    4182,\n",
    "    4183,\n",
    "    4184,\n",
    "    4185,\n",
    "    4186,\n",
    "    4187,\n",
    "    4188,\n",
    "    4189,\n",
    "    4190,\n",
    "    4191,\n",
    "    4192,\n",
    "    4193,\n",
    "    4194,\n",
    "]\n",
    "# 20210323 추가\n",
    "error_list.extend([1516, 1597, 2221, 2808, 2821, 3081, 3084, 3085, 3090, 3093, 3283, 3284])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "divine-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed, deterministic=False):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = deterministic\n",
    "        torch.backends.cudnn.benchmark = not deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reflected-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    AverageMeter, referenced to https://dacon.io/competitions/official/235626/codeshare/1684\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if n > 0:\n",
    "            self.sum += val * n\n",
    "            self.cnt += n\n",
    "            self.avg = self.sum / self.cnt\n",
    "\n",
    "    def get(self):\n",
    "        return self.avg\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-panama",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chicken-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint2box(keypoint, padding=0):\n",
    "    return np.array(\n",
    "        [\n",
    "            keypoint[:, 0].min() - padding,\n",
    "            keypoint[:, 1].min() - padding,\n",
    "            keypoint[:, 0].max() + padding,\n",
    "            keypoint[:, 1].max() + padding,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "possible-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HorizontalFlipEx(A.HorizontalFlip):\n",
    "    swap_columns = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16), (18, 19), (22, 23)]\n",
    "\n",
    "    def apply_to_keypoints(self, keypoints, **params):\n",
    "        keypoints = super().apply_to_keypoints(keypoints, **params)\n",
    "\n",
    "        # left/right 키포인트들은 서로 swap해주기\n",
    "        for a, b in self.swap_columns:\n",
    "            temp1 = deepcopy(keypoints[a])\n",
    "            temp2 = deepcopy(keypoints[b])\n",
    "            keypoints[a] = temp2\n",
    "            keypoints[b] = temp1\n",
    "\n",
    "        return keypoints\n",
    "\n",
    "\n",
    "class VerticalFlipEx(A.VerticalFlip):\n",
    "    swap_columns = [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16), (18, 19), (22, 23)]\n",
    "\n",
    "    def apply_to_keypoints(self, keypoints, **params):\n",
    "        keypoints = super().apply_to_keypoints(keypoints, **params)\n",
    "\n",
    "        # left/right 키포인트들은 서로 swap해주기\n",
    "        for a, b in self.swap_columns:\n",
    "            temp1 = deepcopy(keypoints[a])\n",
    "            temp2 = deepcopy(keypoints[b])\n",
    "            keypoints[a] = temp2\n",
    "            keypoints[b] = temp1\n",
    "\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "checked-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetDataset(Dataset):\n",
    "    def __init__(self, config, files, keypoints, augmentation):\n",
    "        super().__init__()\n",
    "        self.C = config\n",
    "        self.files = files\n",
    "        self.keypoints = keypoints\n",
    "\n",
    "        T = []\n",
    "        T.append(A.Crop(*self.C.dataset.crop))\n",
    "        T.append(A.Resize(self.C.dataset.input_height, self.C.dataset.input_width))\n",
    "        if augmentation:\n",
    "            T_ = []\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=0, p=1))\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=255, p=1))\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=128, p=1))\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=192, p=1))\n",
    "            T_.append(A.Cutout(num_holes=16, max_h_size=100, max_w_size=100, fill_value=64, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=0, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=255, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=128, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=192, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=1920, max_w_size=50, fill_value=64, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=0, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=255, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=128, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=192, p=1))\n",
    "            T_.append(A.Cutout(num_holes=5, max_h_size=30, max_w_size=1080, fill_value=64, p=1))\n",
    "            # T_.append(A.Cutout(max_h_size=20, max_w_size=20))\n",
    "            # T_.append(A.Cutout(max_h_size=20, max_w_size=20, fill_value=255))\n",
    "            # T_.append(A.Cutout(max_h_size=self.C.dataset.input_height // 2, max_w_size=10, fill_value=255))\n",
    "            # T_.append(A.Cutout(max_h_size=self.C.dataset.input_height // 2, max_w_size=10, fill_value=0))\n",
    "            # T_.append(A.Cutout(max_h_size=10, max_w_size=self.C.dataset.input_width // 2, fill_value=255))\n",
    "            # T_.append(A.Cutout(max_h_size=10, max_w_size=self.C.dataset.input_width // 2, fill_value=0))\n",
    "            T.append(A.OneOf(T_))\n",
    "\n",
    "            T.append(A.ShiftScaleRotate(border_mode=cv2.BORDER_CONSTANT))\n",
    "            T.append(HorizontalFlipEx())\n",
    "            T.append(VerticalFlipEx())\n",
    "            # T.append(A.RandomRotate90()) # batch-augmentation으로 대체\n",
    "\n",
    "            T_ = []\n",
    "            T_.append(A.RandomBrightnessContrast())\n",
    "            T_.append(A.RandomGamma())\n",
    "            T_.append(A.RandomBrightness())\n",
    "            T_.append(A.RandomContrast())\n",
    "            T.append(A.OneOf(T_))\n",
    "\n",
    "            T_ = []\n",
    "            T_.append(A.MotionBlur(p=1))\n",
    "            T_.append(A.GaussNoise(p=1))\n",
    "            T.append(A.OneOf(T_))\n",
    "        T.append(A.Normalize())\n",
    "        T.append(ToTensorV2())\n",
    "\n",
    "        self.transform = A.Compose(\n",
    "            transforms=T,\n",
    "            bbox_params=A.BboxParams(format=\"pascal_voc\", label_fields=[\"labels\"]),\n",
    "            # keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = str(self.files[idx])\n",
    "        image = imageio.imread(file)\n",
    "\n",
    "        keypoint = self.keypoints[idx]\n",
    "        box = keypoint2box(keypoint, self.C.dataset.padding)\n",
    "        box = np.expand_dims(box, 0)\n",
    "        labels = np.array([0], dtype=np.int64)\n",
    "        a = self.transform(image=image, labels=labels, bboxes=box)\n",
    "\n",
    "        image = a[\"image\"]\n",
    "\n",
    "        annot = np.zeros((1, 5), dtype=np.float32)\n",
    "        annot[0, :4] = a[\"bboxes\"][0]\n",
    "        annot = torch.tensor(annot, dtype=torch.float32)\n",
    "\n",
    "        return file, image, annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "turkish-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_det_dataset(C, fold):\n",
    "    datadir = Path(C.dataset.dir)\n",
    "    total_imgs = np.array(sorted(list((datadir / \"train_imgs\").glob(\"*.jpg\"))))\n",
    "    df = pd.read_csv(datadir / \"train_df.csv\")\n",
    "    total_keypoints = df.to_numpy()[:, 1:].astype(np.float32)\n",
    "    total_keypoints = np.stack([total_keypoints[:, 0::2], total_keypoints[:, 1::2]], axis=2)\n",
    "\n",
    "    # 오류가 있는 데이터는 학습에서 제외\n",
    "    total_imgs_, total_keypoints_ = [], []\n",
    "    for i in range(len(total_imgs)):\n",
    "        if i not in error_list:\n",
    "            total_imgs_.append(total_imgs[i])\n",
    "            total_keypoints_.append(total_keypoints[i])\n",
    "    total_imgs = np.array(total_imgs_)\n",
    "    total_keypoints = np.array(total_keypoints_)\n",
    "\n",
    "    # KFold\n",
    "    if C.dataset.group_kfold:\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=C.seed)\n",
    "        # 파일 이름 앞 17자리를 group으로 이미지를 분류 (파일이 너무 잘 섞여도 안됨)\n",
    "        groups = []\n",
    "        last_group = 0\n",
    "        last_stem = total_imgs[0].name[:17]\n",
    "        for f in total_imgs:\n",
    "            stem = f.name[:17]\n",
    "            if stem == last_stem:\n",
    "                groups.append(last_group)\n",
    "            else:\n",
    "                last_group += 1\n",
    "                last_stem = stem\n",
    "                groups.append(last_group)\n",
    "        indices = list(skf.split(total_imgs, groups))\n",
    "    else:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=C.seed)\n",
    "        indices = list(kf.split(total_imgs))\n",
    "    train_idx, valid_idx = indices[fold - 1]\n",
    "\n",
    "    # 데이터셋 생성\n",
    "    ds_train = DetDataset(\n",
    "        C,\n",
    "        total_imgs[train_idx],\n",
    "        total_keypoints[train_idx],\n",
    "        augmentation=True,\n",
    "    )\n",
    "    ds_valid = DetDataset(\n",
    "        C,\n",
    "        total_imgs[valid_idx],\n",
    "        total_keypoints[valid_idx],\n",
    "        augmentation=False,\n",
    "    )\n",
    "    dl_train = DataLoader(\n",
    "        ds_train,\n",
    "        batch_size=C.dataset.batch_size,\n",
    "        num_workers=C.dataset.num_cpus,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    dl_valid = DataLoader(\n",
    "        ds_valid,\n",
    "        batch_size=C.dataset.batch_size,\n",
    "        num_workers=C.dataset.num_cpus,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return dl_train, dl_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-hamburg",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baking-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetTrainOutput:\n",
    "    def __init__(self):\n",
    "        self.loss = AverageMeter()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.loss = self.loss()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aboriginal-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetTrainer:\n",
    "    _tqdm_ = dict(ncols=100, leave=False, file=sys.stdout)\n",
    "\n",
    "    def __init__(self, C, fold=1, checkpoint=None):\n",
    "        self.C = C\n",
    "        self.fold = fold\n",
    "\n",
    "        self.det_model = EfficientDet(self.C.det_model.name, pretrained=True)\n",
    "        self.det_model.cuda()\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.AdamW(self.det_model.parameters(), lr=self.C.train.lr)\n",
    "\n",
    "        self.epoch = self.C.train.start_epoch\n",
    "        self.best_loss = math.inf\n",
    "        self.earlystop_cnt = 0\n",
    "\n",
    "        # Dataset\n",
    "        self.dl_train, self.dl_valid = get_det_dataset(C, self.fold)\n",
    "\n",
    "        # Load Checkpoint\n",
    "        if checkpoint is not None:\n",
    "            self.load(checkpoint)\n",
    "\n",
    "        # Scheduler\n",
    "        self.scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer, **self.C.train.scheduler.params)\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": self.det_model.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "                \"epoch\": self.epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"earlystop_cnt\": self.earlystop_cnt,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load(self, path):\n",
    "        print(\"Load pretrained\", path)\n",
    "        ckpt = torch.load(path)\n",
    "        self.det_model.load_state_dict(ckpt[\"model\"])\n",
    "        self.optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        self.epoch = ckpt[\"epoch\"] + 1\n",
    "        self.best_loss = ckpt[\"best_loss\"]\n",
    "        self.earlystop_cnt = ckpt[\"earlystop_cnt\"]\n",
    "\n",
    "    def train_loop(self):\n",
    "        self.det_model.train()\n",
    "\n",
    "        O = DetTrainOutput()\n",
    "        with tqdm(total=len(self.dl_train.dataset), **self._tqdm_, desc=f\"Train {self.epoch:03d}\") as t:\n",
    "            for files, imgs, annots in self.dl_train:\n",
    "                imgs_, annots_ = imgs.cuda(non_blocking=True), annots.cuda(non_blocking=True)\n",
    "\n",
    "                # batch augmentation\n",
    "                if self.C.train.batch_augmentation:\n",
    "                    h, w = imgs.shape[2:]\n",
    "\n",
    "                    # downsample\n",
    "                    if random.random() <= 0.5:\n",
    "                        imgs_ = F.interpolate(imgs_, (h // 2, w // 2))\n",
    "                        annots_[..., :4] *= 0.5\n",
    "\n",
    "                    # rotation\n",
    "                    if random.random() <= 0.5:\n",
    "                        k = random.randint(1, 3)\n",
    "                        a, b, c, d = annots_[..., 0], annots_[..., 1], annots_[..., 2], annots_[..., 3]\n",
    "                        e = annots_[..., 4]\n",
    "                        if k == 1:\n",
    "                            annots_ = torch.stack([b, w - c, d, w - a, e], dim=2)\n",
    "                        elif k == 2:\n",
    "                            annots_ = torch.stack([w - c, h - d, w - a, h - b, e], dim=2)\n",
    "                        elif k == 3:\n",
    "                            annots_ = torch.stack([h - d, a, h - b, c, e], dim=2)\n",
    "                        imgs_ = torch.rot90(imgs_, k=k, dims=(2, 3))\n",
    "\n",
    "                loss = self.det_model(imgs_, annots_)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                O.loss.update(loss.item(), len(files))\n",
    "                t.set_postfix_str(f\"loss: {loss.item():.6f}\", refresh=False)\n",
    "                t.update(len(files))\n",
    "\n",
    "        return O.freeze()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_loop(self):\n",
    "        self.det_model.eval()\n",
    "\n",
    "        O = DetTrainOutput()\n",
    "        with tqdm(total=len(self.dl_valid.dataset), **self._tqdm_, desc=f\"Valid {self.epoch:03d}\") as t:\n",
    "            for files, imgs, annots in self.dl_valid:\n",
    "                imgs_, annots_ = imgs.cuda(non_blocking=True), annots.cuda(non_blocking=True)\n",
    "                loss = self.det_model(imgs_, annots_)\n",
    "\n",
    "                O.loss.update(loss.item(), len(files))\n",
    "                t.set_postfix_str(f\"loss: {loss.item():.6f}\", refresh=False)\n",
    "                t.update(len(files))\n",
    "\n",
    "        return O.freeze()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def callback(self, to: DetTrainOutput, vo: DetTrainOutput):\n",
    "        print(\n",
    "            f\"Epoch: {self.epoch:03d},\",\n",
    "            f\"loss: {to.loss:.6f};{vo.loss:.6f},\",\n",
    "        )\n",
    "\n",
    "        self.scheduler.step(vo.loss)\n",
    "\n",
    "        if self.best_loss > vo.loss:\n",
    "            self.best_loss = vo.loss\n",
    "            self.earlystop_cnt = 0\n",
    "            self.save(self.C.result_dir / f\"effdet_d7_{self.fold}.pth\")\n",
    "        else:\n",
    "            self.earlystop_cnt += 1\n",
    "\n",
    "    def fit(self):\n",
    "        for self.epoch in range(self.epoch, self.C.train.final_epoch + 1):\n",
    "            to = self.train_loop()\n",
    "            vo = self.valid_loop()\n",
    "            self.callback(to, vo)\n",
    "\n",
    "            if self.earlystop_cnt > self.C.train.earlystop_patience:\n",
    "                print(f\"Stop training at epoch\", self.epoch)\n",
    "                break\n",
    "\n",
    "        self.load(self.C.result_dir / f\"effdet_d7_{self.fold}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "likely-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "__effdet_train_config__ = \"\"\"\n",
    "seed: 20210309\n",
    "result_dir: results/submit\n",
    "\n",
    "det_model: \n",
    "  name: efficientdet-d7\n",
    "\n",
    "dataset:\n",
    "  dir: data/ori\n",
    "  batch_size: 2\n",
    "  num_cpus: 1\n",
    "  padding: 20\n",
    "  \n",
    "  crop:\n",
    "    - 192\n",
    "    - 28\n",
    "    - 1728\n",
    "    - 1052\n",
    "\n",
    "  input_width: 768 # 1536\n",
    "  input_height: 512 # 1024\n",
    "  \n",
    "train:\n",
    "  earlystop_patience: 10\n",
    "  start_epoch: 1\n",
    "  final_epoch: 1\n",
    "  \n",
    "  batch_augmentation: true\n",
    "  \n",
    "  folds:\n",
    "    - 1\n",
    "  checkpoints:\n",
    "    - null\n",
    "\n",
    "  lr: 0.0001\n",
    "  scheduler:\n",
    "    type: ReduceLROnPlateau\n",
    "    params:\n",
    "      factor: 0.5\n",
    "      patience: 3\n",
    "      verbose: true\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "brazilian-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    C = EasyDict(yaml.load(__effdet_train_config__, yaml.FullLoader))\n",
    "    fold, checkpoint = C.train.folds[0], C.train.checkpoints[0]\n",
    "\n",
    "    Path(C.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if C.dataset.num_cpus < 0:\n",
    "        C.dataset.num_cpus = cpu_count()\n",
    "\n",
    "    C.result_dir = Path(C.result_dir)\n",
    "    C.dataset.dir = Path(C.dataset.dir)\n",
    "    seed_everything(C.seed)\n",
    "\n",
    "    trainer = DetTrainer(C, fold, checkpoint)\n",
    "    trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hydraulic-chile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained /home/s0/.cache/torch/hub/checkpoints/efficientdet-d7.pth\n",
      "Epoch: 001, loss: 1.374408;0.734192,                                                                \n",
      "Load pretrained results/submit/effdet_d7_1.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/submit/effdet_d7_1.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-d23d51e0bb97>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-aa730a1dd22d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"effdet_d7_{self.fold}.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-aa730a1dd22d>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Load pretrained\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/submit/effdet_d7_1.pth'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-collector",
   "metadata": {},
   "source": [
    "```log\n",
    "[2021-04-04 23:09:27  INFO] Epoch: 001, loss: 1.345929;0.607354,\n",
    "[2021-04-04 23:18:55  INFO] Epoch: 002, loss: 1.032379;0.974972,\n",
    "[2021-04-04 23:28:11  INFO] Epoch: 003, loss: 0.899945;0.447793,\n",
    "[2021-04-04 23:37:33  INFO] Epoch: 004, loss: 0.831420;0.583468,\n",
    "[2021-04-04 23:46:44  INFO] Epoch: 005, loss: 0.799242;1.710531,\n",
    "[2021-04-04 23:56:01  INFO] Epoch: 006, loss: 0.755635;0.422747,\n",
    "[2021-04-05 00:05:15  INFO] Epoch: 007, loss: 0.761829;0.480131,\n",
    "[2021-04-05 00:14:30  INFO] Epoch: 008, loss: 0.680230;2.825914,\n",
    "[2021-04-05 00:23:47  INFO] Epoch: 009, loss: 0.699145;2.252932,\n",
    "[2021-04-05 00:33:00  INFO] Epoch: 010, loss: 0.668748;0.433503,\n",
    "[2021-04-05 00:42:16  INFO] Epoch: 011, loss: 0.559386;0.215368,\n",
    "[2021-04-05 00:51:32  INFO] Epoch: 012, loss: 0.500916;0.258114,\n",
    "[2021-04-05 01:00:47  INFO] Epoch: 013, loss: 0.483150;0.247860,\n",
    "[2021-04-05 01:09:58  INFO] Epoch: 014, loss: 0.458754;0.172292,\n",
    "[2021-04-05 01:19:22  INFO] Epoch: 015, loss: 0.432042;0.156415,\n",
    "[2021-04-05 01:28:40  INFO] Epoch: 016, loss: 0.423682;0.172074,\n",
    "[2021-04-05 01:38:07  INFO] Epoch: 017, loss: 0.419228;0.227202,\n",
    "[2021-04-05 01:47:17  INFO] Epoch: 018, loss: 0.426307;0.202776,\n",
    "[2021-04-05 01:56:38  INFO] Epoch: 019, loss: 0.422774;0.183640,\n",
    "[2021-04-05 02:05:52  INFO] Epoch: 020, loss: 0.356231;0.131375,\n",
    "[2021-04-05 02:15:05  INFO] Epoch: 021, loss: 0.325380;0.121130,\n",
    "[2021-04-05 02:24:25  INFO] Epoch: 022, loss: 0.334707;0.121304,\n",
    "[2021-04-05 02:33:40  INFO] Epoch: 023, loss: 0.315013;0.123282,\n",
    "[2021-04-05 02:43:00  INFO] Epoch: 024, loss: 0.308335;0.137350,\n",
    "[2021-04-05 02:52:19  INFO] Epoch: 025, loss: 0.316076;0.106638,\n",
    "[2021-04-05 03:01:39  INFO] Epoch: 026, loss: 0.303245;0.103056,\n",
    "[2021-04-05 03:10:51  INFO] Epoch: 027, loss: 0.301873;0.114936,\n",
    "[2021-04-05 03:20:15  INFO] Epoch: 028, loss: 0.307543;0.114505,\n",
    "[2021-04-05 03:29:34  INFO] Epoch: 029, loss: 0.298067;0.109576,\n",
    "[2021-04-05 03:38:48  INFO] Epoch: 030, loss: 0.294818;0.132326,\n",
    "[2021-04-05 03:48:07  INFO] Epoch: 031, loss: 0.275761;0.088775,\n",
    "[2021-04-05 03:57:28  INFO] Epoch: 032, loss: 0.264910;0.097282,\n",
    "[2021-04-05 04:06:49  INFO] Epoch: 033, loss: 0.265827;0.094159,\n",
    "[2021-04-05 04:16:11  INFO] Epoch: 034, loss: 0.253544;0.103739,\n",
    "[2021-04-05 04:25:28  INFO] Epoch: 035, loss: 0.264441;0.090745,\n",
    "[2021-04-05 04:34:50  INFO] Epoch: 036, loss: 0.250116;0.080270,\n",
    "[2021-04-05 04:44:12  INFO] Epoch: 037, loss: 0.248722;0.090331,\n",
    "[2021-04-05 04:53:26  INFO] Epoch: 038, loss: 0.243801;0.083525,\n",
    "[2021-04-05 05:02:41  INFO] Epoch: 039, loss: 0.229252;0.083217,\n",
    "[2021-04-05 05:12:03  INFO] Epoch: 040, loss: 0.227173;0.093092,\n",
    "[2021-04-05 05:21:16  INFO] Epoch: 041, loss: 0.233974;0.078002,\n",
    "[2021-04-05 05:30:35  INFO] Epoch: 042, loss: 0.224974;0.077236,\n",
    "[2021-04-05 05:39:51  INFO] Epoch: 043, loss: 0.239731;0.077556,\n",
    "[2021-04-05 05:49:09  INFO] Epoch: 044, loss: 0.231345;0.077701,\n",
    "[2021-04-05 05:58:28  INFO] Epoch: 045, loss: 0.222928;0.079066,\n",
    "[2021-04-05 06:07:48  INFO] Epoch: 046, loss: 0.229059;0.085703,\n",
    "[2021-04-05 06:17:01  INFO] Epoch: 047, loss: 0.238931;0.079773,\n",
    "[2021-04-05 06:26:27  INFO] Epoch: 048, loss: 0.233236;0.077964,\n",
    "[2021-04-05 06:35:47  INFO] Epoch: 049, loss: 0.231080;0.079676,\n",
    "[2021-04-05 06:45:00  INFO] Epoch: 050, loss: 0.225975;0.077771,\n",
    "[2021-04-05 06:54:25  INFO] Epoch: 051, loss: 0.226893;0.070260,\n",
    "[2021-04-05 07:03:43  INFO] Epoch: 052, loss: 0.241337;0.075432,\n",
    "[2021-04-05 07:13:05  INFO] Epoch: 053, loss: 0.226088;0.071070,\n",
    "[2021-04-05 07:22:24  INFO] Epoch: 054, loss: 0.222690;0.075063,\n",
    "[2021-04-05 07:31:41  INFO] Epoch: 055, loss: 0.226015;0.076678,\n",
    "[2021-04-05 07:40:57  INFO] Epoch: 056, loss: 0.228466;0.081537,\n",
    "[2021-04-05 07:50:20  INFO] Epoch: 057, loss: 0.220756;0.076941,\n",
    "[2021-04-05 07:59:38  INFO] Epoch: 058, loss: 0.225289;0.075737,\n",
    "[2021-04-05 08:08:54  INFO] Epoch: 059, loss: 0.233874;0.073094,\n",
    "[2021-04-05 08:18:13  INFO] Epoch: 060, loss: 0.211533;0.076971,\n",
    "[2021-04-05 08:27:30  INFO] Epoch: 061, loss: 0.220242;0.077371,\n",
    "[2021-04-05 08:36:49  INFO] Epoch: 062, loss: 0.220992;0.071223,\n",
    "[2021-04-05 08:36:49  INFO] Stop training at epoch 62\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
